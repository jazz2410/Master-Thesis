\section{Künstliche neuronale Netzwerke}

Wie in Abschnitt \ref{cha:machine_learning} erläutert, bieten sich für die Modellierung des dynamischen Verhaltens der 3D-Servo-Presse neuronale Netze an. Nach \cite{Lipton.5292015} sind diese in der Lage, jeden stetigen nichtlinearen Zusammenhang zwischen Eingangs- und Ausgangsgrößen beliebig genau abzubilden. Es gibt eine Bandbreite von verschiedenen Typen neuronaler Netze, für die nachfolgend ein Überblick gegeben wird. 

\subsection{Feedforward neuronale Netzwerke}
\label{cha_ff}
Die grundsätzliche Funktionsweise von neuronalen Netzen besteht darin, einen mehrdimensionalen Eingabevektor $x = (x_1,x_2,...,x_n)$ in ein Ausgabesignal $\hat{y} = N(x|w)$ zu transformieren. Dabei bezeichnet $w = (w_1,...,w_n)$ die Netzwerkgewichte. Diese nehmen vor der Parametrisierung bzw. vor dem Training des Netzes zunächst zufällige Werte an. Das Multi-Layer-Perceptron (MLP) ist ein weitverbreiteter Netzwerktyp, siehe Abbildung \ref{fig:mlp}. 

\begin{figure} 
	\centering
	\includegraphics[width=0.95\textwidth]{images/MLP}
	\caption{Multi-Layer-Perceptron (MLP) \cite{Velickovic.2018}}
	\label{fig:mlp}
\end{figure}


Bei diesem Netzwerktyp entspricht der Wert eines Neurons einer linearen Überlagerung $n$ sigmoidaler Funktionen, ausgedrückt durch 

\begin{equation} 
\label{eq:feedforward}
\sigma(w_0 + \sum_{i=1}^{n} w_i x_i)
\end{equation}

Die lineare Überlagerung der Netzwerkgewichte $w_1,w_2,...,w_n$ multipliziert mit den Eingangsgrößen $x_1,x_2,...,x_n$ und verschoben um den Wert $w_0$ wird nochmals durch eine Aktivierungsfunktion transformiert. Mögliche Aktivierungsfunktionen sind die \textit{logistische Sigmoid}-Funktion $\sigma(z) = \frac{1}{1 + e^{-z}}$, die \textit{tangentiale hyperbolische Sigmoid}-Funktion $\phi(z)=\frac{2}{1 + e^{-2z}}-1$ und die \textit{lineare} Funktion $a(z)=z$. \\ 
Die Auswahl der Aktivierungsfunktion ist hierbei vom konkreten Anwendungsfall abhängig. Das Training bzw. die Parametrisierung des Netzes erfolgt nun in zwei Schritten. Im ersten Schritt findet eine Übergabe der Eingangsgrößen $x_1,x_2,...,x_n$ an das Netz statt. Weiterhin nehmen die Netzwerkgewichte $w$ anfangs zufällige Werte an. Da die Eingangsgrößen und die Netzwerkgewichte bekannt sind, kann jeder Neuronenwert nach Gleichung \ref{eq:feedforward} berechnet werden. Die nachfolgenden Neuronenwerte der nächsten Schicht lassen sich somit sukzessive als Funktion der vorherigen Schicht berechnen. Auf diese Weise des Vorwärtsrechnens findet eine Transformation der Eingabegrößen $x_1,x_2,...x_n$ in ein Ausgangssignal $\hat{y}$ statt (in Abbildung \ref{fig:mlp} als $O_1$ und $O_2$ bezeichnet). \\
Der zweite Schritt umfasst nun die Adaptierung der Netzwerkgewichte, sodass die vom Netz herausgegebenen Ausgabegrößen $\hat{y}$ mit den gemessenen Ausgabegrößen $y_i$ möglichst übereinstimmen. Dabei ist es zweckmäßig, die Abweichung zwischen diesen Größen in Form der Fehlerfunktion 

\begin{equation}
E(w) = (y_i-N(x|w))^{2}
\end{equation}

zu beschreiben. Die Adaptierung der Netzwerkgewichte beschränkt sich nun darauf, die Fehlerfunktion zu minimieren. Eine der gängigsten Methoden ist die Methode des Gradientenabstieges. Diese gibt als Adaptionsregel für die Netzwerkgewichte 

\begin{equation}
\label{eq:weight_adapt}
\Delta w_i = \epsilon  (-\nabla E_i) = \epsilon * - \dfrac{\partial E}{\partial w}\bigg|_{w_i}
\end{equation}

mit $\epsilon$ als Lernrate bzw. Adaptionsschrittweite vor. Das Ziel besteht darin, für die Fehlerfunktion $E(w)$ die partiellen Ableitungen $\dfrac{\partial E}{\partial w}\bigg|_{w_i}$ zu identifizieren und gleich null zu setzen. Damit würde die Fehlerfunktion $E(w)$ ein lokales Minima erreichen. Dies geschieht durch die lokale Suche nach dem Minimum der Fehlerfunktion $E(w)$ in Richtung des steilsten Abstiegs. Die Richtung des steilsten Abstiegs entspricht dabei dem negativen Gradienten von $E$ nach den Netzwerkgewichten $w_i$.\\ 
Die Adaption findet für jedes vorhandene Datenpaar $((x_1,x_2,...,x_n)|(y_1))$ statt. Durch das Wiederholen des Adaptionsvorgang passt sich das Netz an das zu modellierende Systemverhalten an. \\

Wie bereits erwähnt, bietet sich die Methode des Gradientenabstieges als ein mögliches Verfahren zur Minimierung der Fehlerfunktion an. Es gibt darüber hinaus in der Literatur eine Vielzahl von Verfahren, die sich nach \cite{Sklyarenko.2002} in folgende Kategorien einteilen lassen:

\begin{itemize}
	\item Gradientenverfahren erster Ordnung
	\item Gradientenverfahren zweiter Ordnung
	\item stochastische Optimierungsverfahren
	\item Verfahren der globalen Optimierung
\end{itemize}


\textbf{Backpropagation-Algorithmus}

Den Verfahren der ersten Kategorie ist gemein, dass die Berechnung des Gradienten des Gütefunktionals

\begin{equation}
\nabla E_i = \dfrac{\partial E}{\partial w}\bigg|_{w_i}
\end{equation}

in jeder Iteration problematisch ist, da sich die Gradienten nicht in einem Schritt berechnen lassen. In der Regel kommt für die Lösung dieses Problems der \textit{Backpropagation}-Algorithmus zum Einsatz. Dieser wendet die Kettenregel an, um die Gradienten in mehrere Faktoren, bestehend aus partiellen Ableitungen, zu zerlegen. Auf diese Weise können die partiellen Ableitungen der letzten Schicht, welche in der Regel bekannt sind, benutzt werden können, um die partiellen Ableitungen der vorangehenden Schichten zu bestimmen. Dies ist gleichbedeutend mit dem Belegen der \textit{Jacobi-Matrix}. \\


\begin{figure} 
	\centering
	\includegraphics[width=0.65\textwidth]{images/backpropagation2}
	\caption{Multi-Layer-Perceptron \cite{Sturm.2000}}
	\label{fig:backpropagation}
\end{figure}

Die Ermittlung der notwendigen partiellen Ableitungen ergibt sich nach \cite{Sturm.2000} durch folgende Rekursionsformeln \\

\begin{equation}
\begin{aligned}
Y'_{i-1} = Y'_i \cdot S'_i \cdot W_i \\
\dfrac{\partial y_N}{\partial (W_i)_{jk}} = Y'_i \cdot S'_i \cdot \left(\begin{array}{c}
(y_{i-1})_k  \\ ... \\ ... \\ (y_{i-1})_k 
\end{array}\right) \\
\dfrac{\partial y_N}{\partial \vec{b_i}} = Y'_i \cdot S'_i
\end{aligned}
\end{equation}

Dabei enthält die Matrix $Y'_i$ die partiellen Ableitungen der Ausgabegrößen der letzten, also der $N$ten Schicht, nach der $i$ten Schicht. Am Anfang der Rekursion entspricht diese der Einheitsmatrix. Die Matrix $S'_i$ enthält die Ableitungen der Aktivierungsfunktionen der $i$ten Schicht. \cite{Sturm.2000}


Die Ermittlung der partiellen Ableitungen findet also von der letzten bis zur ersten Schicht, also rückwärts, statt. Nach Ermittlung der partiellen Ableitungen aller Schichten, kann die Adaption der Netzwerkgewichte gemäß einer Adaptionsregel, siehe \ref{eq:weight_adapt}, erfolgen. Während des Adaptionsprozesses nimmt der Gesamtfehler ab. Sinkt der Gesamtfehler über alle Netzausgänge $y_1,...,y_m$ unter eine festgelegte Grenze, bricht das Training des Netzes ab. \\


Es bleibt festzuhalten, dass die Anpassungsfähigkeit des Netzes an das nichtlineare Verhalten dynamischer Systeme von folgenden Faktoren abhängt:

\begin{itemize}
	\item Anzahl der Neuronen
	\item Anzahl der Schichten
	\item Wahl des Algorithmus zur Minimierung der Fehlerfunktion
	\item Auswahl der Aktivierungsfunktionen
	\item Initialisierung der Gewichte
	\item Repräsentationsfähigkeit der Trainingsdaten
	
\end{itemize}

Die richtige Auslegung der Parameter für das neuronale Netz stellt eine Herausforderung dar. In Abschnitt \ref{cha:ff_herausforderung} ist eine ausführliche Diskussion über die Auswahl der Netzparameter, vor allem über den Algorithmus und  die Anzahl der Neuronen, enthalten. \\ 

\subsubsection{Herausforderungen bei der Auslegung neuronaler Netze}
\label{cha:ff_herausforderung}

\textbf{Anzahl der Neuronen und Gewichte}

Für die Auslegung neuronaler Netze ist die Abschätzung der Neuronenanzahl relevant. Dabei ist die Festlegung der optimalen Neuronenanzahl eine Abwägung zwischen der Traningszeit, der Generalisierungs- und der Anpassungsfähigkeit an die Nichtlinearitäten des dynamischen Systems. Beispielsweise erhöht nach \cite{Fumumoto.2017} die Anzahl der Neuronen in der verdeckten Schicht die Güte des neuronalen Netzes bei einem Anstieg nichtlinearer Effekte. \\
Nach \cite{.2008} können ebenfalls heuristische Aussagen über eine minimale und maximale Neuronenanzahl getroffen werden. Diese bieten allerdings nur Abschätzungen an. Weiterhin gibt es Vorschläge, die Neuronenanzahl während des Trainings zu adaptieren.\\ 
Dafür lassen sich die Verfahren in zwei Gruppen aufteilen, einmal in konstruktive und reduzierende Verfahren. \\ 
Bei den reduzierenden Verfahren ist die Ausgangsbasis ein bereits trainiertes Netz, welches ein System ausreichend  genau modelliert. \\ 
Nach \cite{Han.682015} sind viele neuronale Netze über-parametrisiert, d.h. sie verfügen über redundante Gewichte. Eine Möglichkeit der Reduktion besteht nun darin, sämtliche Gewichte, deren Wert unter einer vorher definierten Grenze liegt, zu entfernen. Es findet somit eine Unterscheidung zwischen bedeutenden und unbedeutenden Gewichte statt. Nach einer erneuten Trainingsphase findet eine erneute Adaptierung der verbliebenen Netzwerkgewichte statt, sodass diese die erzeugte Abweichung durch das Entfernen der unbedeutenden Gewichte kompensieren können. \cite{Han.682015} \\
Der Einsatz reduzierender Verfahren ist mit Nachteilen verbunden. Zum einen muss ein bereits über-parametrisiertes Netz vorliegen. Die Trainingszeit bei über-parametrisierten Netzen ist in Regel höher also bei nicht über-parametrisierten Netzen. Der Reduktionsvorgang der neuronalen Netze stellt somit einen weiteren Arbeitsschritt dar, dessen zeitlicher Aufwand sich mit dem bereits erhöhten zeitlichen Aufwand für das Training des über-parametrisierten Netzes überlagert. \\
Konstruktive Ansätze dagegen verfolgen den Ansatz, durch Algorithmen ein anfangs mit einer minimalen Topologie ausgestattetes Netz inkrementell mit einer höheren Anzahl an Neuronen und Gewichten auszustatten, wobei jede Modifikation auf ihre Güte untersucht wird. Nach \cite{Parekh.2000} kann dabei durch die Auswahl des Algorithmus erzwungen werden, dass jede weitere Netztopologie einen geringeren Fehler aufweist als die Vorgängertopologie. \\
Der Vorteil bei konstruktiven Ansätzen liegt darin, dass das Entstehen über-parametrisierter Netze von Anfang an unterbunden wird. Über-parametrisierte Netze neigen zum sogenannten \textit{Overfitting}. Beim \textit{Overfitting} sind Netze nicht in der Lage, die Zusammenhänge zwischen Eingangs- und Ausgangsgrößen ausreichend zu verallgemeinern bzw. zu generalisieren. Sie sind zwar in der Lage, den Trainingsdatensatz gut abzubilden, aber nicht in der Lage, für einen unbekannten Datensatz die Ausgangsgröße gut genug zu approximieren. Das gleiche Problem tritt beim \textit{Underfitting} auf. Dabei ist das Netz unter-parametriesiert, d.h. es verfügt nicht über genug Parameter (Netzwerkgewichte), um die Zusammenhänge zwischen Eingags- und Ausgangsgrößen gut genug abzubilden.


\textbf{Trainingsalgorithmen}

Wie in Abschnitt \ref{cha_ff} erwähnt, ist die Methode des Gradientenabstieges eine Möglichkeit der Gewichtsadaption. Jedoch ist diese Methode im Bereich der Gewichtsadaption für neuronale Netze zum größten Teil durch fortschrittlichere Methoden abgelöst worden. Die Limitierungen der Methode des Gradientenabstieges und alternative Algorithmen (z.B. konjugierte Gradienten-Methode, David-Fletcher-Powell-Algorithmus) sind in \cite{WarrenS.Sarle.1994}, \cite{Masters.1995} und \cite{Bishop.2010} sehr gut dokumentiert. Eine weitere Alternative stellt der Levenberg-Marquadt-Algorithmus dar, dessen Implementierung in \cite{Kollias.March1993} diskutiert ist. Weitere Trainingsverfahren sind genetische Algorithmen, deren Einführung in \cite{Hassoun.1996} erörtert worden ist. Diese kommen nicht nur für die Adaptierung der Gewichte, sondern auch für die Adaptierung der Netzwerktopologie (Anzahl der Schichten, Anzahl der Neuronen, Auswahl der Aktivierungsfunktionen, etc.) zum Einsatz, siehe \cite{Bayer.2009} und \cite{StevenA.Harp.1992}.  \\
Weitere Möglichkeiten sind stochastische Methoden des Gradientenabstieges (stochastic gradient descent oder SGD), welche vor allem das Ziel haben, den Trainingsvorgang zu beschleunigen. \cite{JohnDuchi.2010} und \cite{Zeiler.12222012} verändern dafür die Adaptions- bzw. Trainingsrate während der Trainingsphase. Dies führt zu schnelleren bei konvexen, aber zu geringeren Konvergenzgeschwindigkeiten bei nicht-konvexen Fehlerfunktionen. 

Bei der Auswahl von Algorithmen spielt außerdem  die Anzahl der Netzwerkgewichte eine Rolle. Nach \cite{WarrenS.Sarle.1994} eignet sich der Levenberg-Marquardt-Algorithmus bei mehreren Zehnten, der Davidon-Fletscher-Power-Algorithmus bei mehreren Hunderten und der konjugierte Gradienten-Algorithmus bei mehreren Tausenden von Netzwerkgewichten. 


\textbf{Aktivierungsfunktionen} 

Wie in Abschnitt \ref{cha_ff} erwähnt, gibt es für die Auswahl der Aktivierungsfunktion mehrere Möglichkeiten (u.a. die \textit{logistische Sigmoid-}, die \textit{tangentiale hyperbolische Sigmoidfunktion} und die \textit{lineare} Funktion). Während die \textit{lineare} Funktion die Eingangsgröße linear in die Ausgangsgröße transformiert, besteht die Aufgabe der \textit{logistischen Sigmoid-} und der \textit{tangentialen hyperbolischen Sigmoid}- Funktion darin, Nichtlinearitäten in das Netz zu induzieren. So transformiert die \textit{logistische Sigmoid}-Funktion die Eingangsgröße in einen Wertebereich zwischen 0 und 1 und die \textit{tangentiale hyperbolische Sigmoid}-Funktion die Eingangsgröße in einen Wertebereich zwischen -1 und 1. Generell eignet sich jedoch jede differenzierbare Funktion als Aktivierungsfunktion. Die Auswahl der Aktivierungsfunktion der letzten Schicht sollte jedoch auf den Wertebereich der Ausgangsgröße abgestimmt sein. Befindet sich der Wertebereich der Ausgangsgrößen nicht nur in einem Bereich zwischen -1 und 1, kommen dafür in der Regel \textit{lineare} Aktivierungsfunktionen in der letzten Schicht, auch Ausgabeschicht genannt, zum Einsatz. Unterscheidungsmerkmal ist nicht nur der Wertebereich der Ausgangsgröße, sondern auch der Typ der Ausgangsgröße, welcher numerisch, kategorisch oder binär sein kann.  Weiterhin haben die Aktivierungsfunktionen einen Einfluss auf die Konvergenzgeschwindigkeit bzw. Trainingsgeschwindigkeit des Netzes. Nach \cite{Bishop.2010} weisen Netze mit \textit{tangentialen hyperbolischen Sigmoid}-Funktionen höhere Konvergenzzeiten auf als Netze mit \textit{logistischen Sigmoid}-Funktionen. In \cite{Kalman.June1992} dagegen sind die Vorzüge der \textit{tangentialen hyperbolischen Sigmoid}-Funktion gegenüber anderen Aktivierungsfunktionen in Bezug auf mathematischen Eigenschaften während der Trainingsphase diskutiert.

\textbf{Initialisierung der Gewichte}

Wie in Abschnitt \ref{cha_ff} beschrieben, nehmen die Gewichte am Anfang der Trainingsphase zufällige Werte an. Erst im Laufe des Trainings findet die Adaption der Gewichte durch das Minimieren der Fehlerfunktion statt. Dieses findet durch das Anwenden verschiedener Algorithmen (Methode des Gradientenabstieges, Levenberg-Marquardt-Algorithmus etc.) statt. Diese sind allerdings nicht in der Lage, das globale Minimum der Fehlerfunktion zu identifizieren. In der Regel finden die Algorithmen lediglich lokale Minima der Fehlerfunktion. Somit können zwei neuronale Netze, welche mit unterschiedlichen Gewichten initialisiert wurden, aber deren Gewichte mit dem gleichen Algorithmus adaptiert wurden, unterschiedlichen Güten aufweisen, da der Trainingsprozess bei unterschiedlichen lokalen Minima abbricht. In der Literatur finden sich heuristische Ansätze, die initialen Werte für die Gewichte festzulegen. Zum Beispiel wählt \cite{Piovoso.1991b} die initialen Gewichte zwischen der Eingabeschicht und der verdeckten Schicht auf Basis einer Komponentenanalyse und die initialen Gewichte zwischen der verdeckten Schicht und der letzten Schicht auf Basis einer multiplen linearen Regression aus. 

\textbf{Anzahl der Schichten}

Die Netzwerktopologie ist ebenfalls durch die Anzahl der Schichten bestimmt. Nach \cite{Hornik.1989} ist jedoch ein Netz mit nur einer verdeckten Schicht in der Lage, fast jede beliebige Funktion zu approximieren. Die Notwendigkeit einer zweiten Schicht kann sich dann ergeben, wenn stückweise stetige Funktionen zu approximieren sind.

\textbf{Schlussfolgerung}

Das bisher vorgestellte \textit{Feedforward-Netz} ist vor allem dazu geeignet, den statischen Zusammenhang zwischen Eingangs- und Ausgangsgrößen abzubilden. \textit{Feedforward-Netze} sind rein vorwärts gerichtete Netze und definieren sich lediglich über die Eingabegrößen und die Parametrisierung der Gewichte. Somit sind \textit{Feedforward-Netze} zustandsfrei. Davon unterscheiden sich sogenannte \textit{rekurrente Netze}, welche den Einfluss der Zeit zusätzlich berücksichtigen können. 





\subsection{Rekurrente neuronale Netze}

Bei \textit{rekurrenten Netzen} findet - im Gegensatz zu \textit{Feedforward-Netzen} - eine Berücksichtigung des zeitlichen Verlaufs der Eingabe- und Ausgabegrößen statt. Dies wird durch den Einsatz von \textit{rekurrenten Kanten} ermöglicht, welche zwei neuronale Netze in aufeinanderfolgenden Zeitschritten miteinander verbinden, siehe Abbildung \ref{fig:recurrent}.  

\begin{figure} [h]
	\centering
	\includegraphics[width=0.65\textwidth]{images/recurrent_net}
	\caption{Rekurrentes Netz \cite{Lipton.5292015}}
	\label{fig:recurrent}
\end{figure}

Die \textit{rekurrenten Kanten} erlauben es, dass die Netzwerkneuronen zum Zeitpunkt $t$ nicht nur die Eingangsgröße $x(t)$, sondern auch die Ausgangsgröße $h^{(t-1)}$ der verdeckten Neuronen des vorherigen Zeitschrittes zum Eingang haben. Der Ausgang des Netzes $\hat{y}^{(t)}$ zum Zeitpunkt $t$ ergibt sich aus dem Ausgang $h^{(t)}$ aller verdeckten Neuronen zum Zeitpunkt $t$. Auf diese Weise beeinflusst die Eingangsgröße $x^{(t-1)}$ zum Zeitpunkt $t-1$ die Ausgangsgröße $\hat{y}^{(t)}$ zum Zeitpunkt $t$. Der Ausgang der verdeckten Neuronen $h^{(t)}$ zum Zeitpunkt $t$ kann nach \cite{Lipton.5292015} durch 

\begin{equation}
h^{(t)} = \sigma(W^{hx}x^{(t)} + W^{hh}h^{(t-1)} + b_h)
\end{equation}

beschrieben werden. Dabei besteht die Matrix $W^{hx}$ aus den konventionellen Gewichten, welche sich zwischen den Eingangsgrößen und den verdeckten Neuronen befinden. Die Matrix $W^{hh}$ dagegen besteht aus \textit{rekurrenten Gewichten}, welche sich zwischen den verdeckten Neuronen des aktuellen und des vorangegangenen Zeitschrittes befinden. Für die Adaptierung der Netzwerkgewichte kommt bei \textit{rekurrenten Netzen} üblicherweise der \textit{Backpropagation-Through-Time(BBTT)}- oder der \textit{Real-Time Recurrent Learning(RTRL)}-Algorithmus zum Einsatz. Der Einsatz dieser Algorithmen ist jedoch mit Problemen verbunden, siehe Abschnitt \ref{cha_recurrent_problem}. \\ 
Im Laufe der Zeit haben sich unterschiedliche Varianten \textit{rekurrenter Netze} entwickelt. Zunächst seien frühere Architekturen \textit{rekurrenter Netze} erläutert.


\textbf{Jordan-Netz}

Das \textit{Jordan-Netz} stellt eine der frühen rekurrenten Netzarchitekturen dar, siehe Abbildung \ref{fig:jordan}. 

\begin{figure} [h]
	\centering
	\includegraphics[width=0.65\textwidth]{images/jordan}
	\caption{Jordan-Netz \cite{Lipton.5292015}}
	\label{fig:jordan}
\end{figure}


Das \textit{Jordan}-Netz entspricht einem \textit{Feedforward}-Netz mit einer nachgeschalteten Zustandsschicht, bestehend aus Zustandsneuronen. Diese sind über feste Gewichte (in der Regel haben diese den Wert 1) mit den Neuronen der Ausgangsschicht verbunden. Im nächsten Zeitschritt übergeben diese ihre Ausgangsgröße an die Neuronen der verdeckten Schicht. Durch die festen Gewichte der \textit{rekurrenten} Kanten findet eine Speicherung der Ausgangsgrößen aus dem letzten Zeitschritt in den Zustandsneuronen statt. Auf diese Weise ist das Netz in der Lage Zustände zu speichern, wozu konventionelle \textit{Feedforward-Netze} nicht in der Lage sind. Der Ausgang der verdeckten Schicht $h^{(t)}$ zum Zeitpunkt $t$ ergibt sich nach \cite{Jordan.1997} durch

\begin{equation}
h^{(t)} = \sigma(W^{hx}x^{(t)} + W^{hh}y^{(t-1)} + b^{h})
\end{equation}

Dabei enthält die Matrix $W^{hx}$ die Gewichte zwischen den Eingangsgrößen und den Neuronen in der verdeckten Schicht. Die Matrix $W^{hh}$ dagegen enthält die konstanten Gewichte der \textit{rekurrenten Kanten}. Der Unterschied zu moderneren \textit{rekurrenten Netzen} besteht darin, dass das Jordan-Netz wie ein \textit{Feedforward}-Netz mittels des \textit{Backpropagation}-Algorithmus trainiert wird, wobei die Gewichte der \textit{rekurrenten} Kanten ignoriert bzw. nicht adaptiert werden. Dabei findet - im Gegensatz zu modernen \textit{rekurrenten Netzen} - keine Rückführung des Fehlers über verschiedene Zeitschritte statt. Damit ist dieses Netz nur eingeschränkt für komplexere Dynamiken einsetzbar. Ein weiterer Nachteil ist, dass die Anzahl der \textit{rekurrenten Kanten} durch die Anzahl der Ausgabeneuronen, also durch die Problemstellung, festgelegt ist. Diesen Nachteil umgeht das \textit{Elman-Netz}.

\textbf{Elman-Netz}

Das \textit{Elman-Netz} stellt ebenfalls eine der früheren \textit{rekurrenten} Netzarchitekturen dar. Der entscheidende Unterschied zum \textit{Jordan-Netz} ist, dass die \textit{rekurrenten Kanten} nicht wie beim \textit{Jordan-Netz} die Ausgabeneuronen mit den Zustandsneuronen, sondern die verdeckten Neuronen mit den Zustandsneuronen verbinden, siehe Abbildung \ref{fig:elman}.

\begin{figure} [h]
	\centering
	\includegraphics[width=0.65\textwidth]{images/elman}
	\caption{Elman-Netz \cite{Lipton.5292015}}
	\label{fig:elman}
\end{figure}

Das \textit{Elman-Netz} ist wie das \textit{Jordan-Netz} in der Lage, Zustände zu speichern, indem die Zustandsneuronen den Zustand des vorherigen Zeitschrittes im nächsten Zeitschritt an die verdeckten Neuronen übergeben. Somit ergibt sich der Ausgang der verdeckten Schicht $h^{(t)}$ zum Zeitpunkt $t$ nach \cite{Elman.1990} durch 

\begin{equation}
h^{(t)} = \sigma(W^{hx}x^{(t)} + W^{hh}h^{(t-1)} + b^{h})
\end{equation}

Auch hier enthält die Matrix $W^{(hx)}$ die Gewichte zwischen den Eingangsgrößen und den verdeckten Neuronen und die Matrix $W^{(hh)}$ die konstanten Gewichte der \textit{rekurrenten Kanten}.
Wie bei den \textit{Jordan-Netzen} findet das Training des Netzes über den \textit{Backpropagation-Algorithmus} statt, wobei die konstanten Gewichte der \textit{rekurrenten Kanten} ignoriert bzw. nicht adaptiert werden. 
Somit ergibt sich auch beim \textit{Elman-Netz} das Problem, dass keine Rückführung des Fehlers über die Zeit stattfindet, sodass die Anwendbarkeit für komplexere Dynamiken beschränkt ist. Beim \textit{Elman-Netz} hängt jedoch die Anzahl der \textit{rekurrenten Kanten} nicht von der Anzahl der Ausgabeneuronen, sondern von der Anzahl der verdeckten Neuronen ab. Da die Anzahl der Neuronen in der verdeckten Schicht frei ausgewählt werden kann, hängt die Anzahl der \textit{rekurrenten Kanten} nicht von der Aufgabenstellung ab. Dies ist ein Vorteil gegenüber dem \textit{Jordan-Netz}.

 
\subsubsection{Herausforderungen bei rekurrenten Netzen}
\label{cha_recurrent_problem}

\textbf{Verschwindende und explodierende Gradienten}

\textit{Rekurrente Netze} können - im Gegensatz zu \textit{Feedforward-Netzen} - durch den Einsatz \textit{rekurrenter Kanten} Informationen speichern. Jedoch ist die Anwendung bestimmter Algorithmen für das Netztraining mit Problemen verbunden. \\
Nach \cite{Hochreiter.2001} ist der Einsatz von gradientenbasierten Trainingsalgorithmen, zu denen der \textit{Backpropagation-Through-Time}- und der \textit{Real-Time Recurrent}-Algorithmus gehören, beim Training \textit{rekurrenter Netze} problematisch. Beim  Netztraining können dabei zwei Effekte auftreten. Der erste Effekt umfasst das \textit{Verschwinden der Gradienten}. Dabei nehmen die Gradienten verschwindend kleine Werte an. Dies führt dazu, dass die Adaptierung der Gewichte, also das Training des Netzes, einen sehr hohen Zeitaufwand erfordert oder überhaupt nicht mehr stattfindet. Der zweiter Effekt umfasst das \textit{Explodieren der Gradienten}. Dabei nehmen die Gradienten sehr hohe Werte an, was zu oszillierenden Gewichten und somit zu einer schlechteren Robustheit des Netzes führt. \cite{Hochreiter.2001} \\
\cite{Hochreiter.2001} schlägt vor, das Problem der \textit{verschwindenden Gradienten} durch eine neue Netzstruktur zu lösen. Bei dieser Netzstruktur kommen sogenannte Long Short-Term Memory(LSTM)-Zellen zum Einsatz, welche die konventionellen verdeckten Knoten ersetzen, siehe Abschnitt. 

Die mathematischen Hintergründe und die Charakterisierung der Rahmenbedingungen, welche zu \textit{verschwindenden} oder zu \textit{explodierenden} Gradienten führt, sind in \cite{Pascanu.11212012} erläutert. \cite{Pascanu.11212012} schlägt vor, den Wertebereich der Gewichte durch Regularisierung einzuschränken. Der Einschränkung des Wertebereiches findet derart statt, dass eine zu große Veränderung des Gradienten unterbunden ist. \\


\textbf{Minimierung der Fehlerfunktion}

Wie bei den \textit{Feedforward-Netzen} tritt bei den \textit{rekurrenten Netzen} das Problem auf, dass das Auffinden des globalen Minima der Fehlerfunktion kaum möglich ist. \\
Die Herausforderung besteht darin, durch die Adaption der Gewichte ein möglichst tiefes lokales Minima der Fehlerfunktion zu besetzen. Nach \cite{Dauphin.6102014} sollte bei höher dimensionalen nicht-konvexen Fehlerfunktionen Sattelpunkten eine viel höhere Bedeutung zukommen als lokalen Minima, da die Anzahl der kritischen lokalen Minima bei größeren Netzen exponentiell sinkt. Demnach wird von \cite{Dauphin.6102014} ein Sattelpunkt-freier Newton-Algorithmus vorgeschlagen. Diese Auslegung des Algorithmus unterscheidet sich von der konventionellen Newton-Methode darin, dass die Gewichtsadaptierung derart stattfindet, dass der Wert der Fehlerfunktion sich möglichst weit weg von den kritischeren Sattelpunkten befindet. Es konnten damit zwar Verbesserungen in Hinblick auf die Güte von \textit{rekurrenten Netzen} erzielt werden, allerdings ist für die Anwendung des Sattelpunkt-freien Newton-Methode die rechenintensive Ermittlung der Hesse-Matrix notwendig. \cite{Dauphin.6102014}  



   
 
 
 
