% This file was created with Citavi 6.1.0.0

@incollection{.2008,
 title = {Appendix G: Thirty Years of Adaptive Neural Networks: Perceptron, Madaline, and Backpropagation},
 pages = {409--474},
 publisher = {{IEEE Press Wiley-Interscience} and {IEEE Xplore}},
 isbn = {9780470231616},
 series = {IEEE Press Series on Power Engineering},
 editor = {Widrow, Bernard and Walach, Eugene},
 booktitle = {Adaptive inverse control},
 year = {2008},
 address = {Piscataway, New Jersey and Hoboken, NJ and Piscataway, New Jersey},
 doi = {10.1002/9780470231616.app7}
}


@article{Alexandridis.2005,
 author = {Alexandridis, Alex and Sarimveis, Haralambos},
 year = {2005},
 title = {Nonlinear adaptive model predictive control based on self-correcting neural network models},
 pages = {2495--2506},
 volume = {51},
 number = {9},
 issn = {0001-1541},
 journal = {AIChE Journal},
 doi = {10.1002/aic.10505}
}


@book{Alippi.2009,
 year = {2009},
 title = {Artificial neural networks - ICANN 2009: 19th international conference, Limassol, Cyprus, September 14 - 17, 2009 ; proceedings, part II},
 keywords = {Artificial intelligence;Computer science;Computer simulation;Data mining;ilmpub;Neurosciences;Optical pattern recognition},
 address = {Berlin},
 volume = {5769},
 publisher = {Springer},
 isbn = {978-3-642-04276-8},
 series = {Lecture Notes in Computer Science},
 editor = {Alippi, Cesare and Polycarpou, Marios and Panayiotou, Christos and Ellinas, Georgios},
 doi = {10.1007/978-3-642-04277-5}
}


@article{Almusawi.2016,
 abstract = {This paper presents a novel inverse kinematics solution for robotic arm based on artificial neural network (ANN) architecture. The motion of robotic arm is controlled by the kinematics of ANN. A new artificial neural network approach for inverse kinematics is proposed. The novelty of the proposed ANN is the inclusion of the feedback of current joint angles configuration of robotic arm as well as the desired position and orientation in the input pattern of neural network, while the traditional ANN has only the desired position and orientation of the end effector in the input pattern of neural network. In this paper, a six DOF Denso robotic arm with a gripper is controlled by ANN. The comprehensive experimental results proved the applicability and the efficiency of the proposed approach in robotic motion control. The inclusion of current configuration of joint angles in ANN significantly increased the accuracy of ANN estimation of the joint angles output. The new controller design has advantages over the existing techniques for minimizing the position error in unconventional tasks and increasing the accuracy of ANN in estimation of robot's joint angles.},
 author = {Almusawi, Ahmed R. J. and D{\"u}lger, L. Canan and Kapucu, Sadettin},
 year = {2016},
 title = {A New Artificial Neural Network Approach in Solving Inverse Kinematics of Robotic Arm (Denso VP6242)},
 keywords = {Arm/physiology;Biomechanical Phenomena/physiology;Female;Humans;Male;Neural Networks (Computer);Robotics;User-Computer Interface},
 pages = {5720163},
 volume = {2016},
 journal = {Computational intelligence and neuroscience},
 doi = {10.1155/2016/5720163}
}


@proceedings{AmericanControlConference.1995,
 year = {1995},
 title = {Proceedings of the 1995 American Control Conference: The Westin Hotel, Seattle, Washington, June 21-June 23, 1995},
 keywords = {Automatic control;Congresses;Digital control systems;Guidance systems (Flight)},
 address = {Piscataway, N.J},
 publisher = {{Distributed through the IEEE Service Center}},
 isbn = {0-7803-2445-5},
 institution = {{American Control Conference} and {American Automatic Control Council} and {International Federation of Automatic Control}}
}


@article{Andersen.1990,
 author = {Andersen, K. and Cook, G. E. and Karsai, G. and Ramaswamy, K.},
 year = {1990},
 title = {Artificial neural networks applied to arc welding process modeling and control},
 pages = {824--830},
 volume = {26},
 number = {5},
 issn = {00939994},
 journal = {IEEE Transactions on Industry Applications},
 doi = {10.1109/28.60056}
}


@book{Arkun.1991,
 year = {1991},
 title = {Chemical process control-CPCIV: Proceedings of the Fourth International Conference on Chemical Process Control, Padre Island, Texas, February 17-22, 1991},
 keywords = {Chemical;Congresses},
 address = {Austin Tex.},
 volume = {P-67},
 publisher = {CACHE},
 isbn = {0816905495},
 series = {AIChE publication},
 editor = {Arkun, Yaman and Ray, Willis Harmon}
}


@proceedings{Barrett.1991,
 year = {1991},
 title = {Proceedings of the conference on Analysis of neural network applications},
 keywords = {Computer science},
 address = {New York, NY},
 publisher = {ACM},
 isbn = {0897914325},
 editor = {Barrett, Larry K.},
 institution = {{ACM Special Interest Group on Management Information Systems} and {ACM Special Interest Group on Artificial Intelligence}},
 doi = {10.1145/106965}
}


@incollection{Bayer.2009,
 author = {Bayer, Justin and Wierstra, Daan and Togelius, Julian and Schmidhuber, J{\"u}rgen},
 title = {Evolving Memory Cell Structures for Sequence Learning},
 pages = {755--764},
 volume = {5769},
 publisher = {Springer},
 isbn = {978-3-642-04276-8},
 series = {Lecture Notes in Computer Science},
 editor = {Alippi, Cesare and Polycarpou, Marios and Panayiotou, Christos and Ellinas, Georgios},
 booktitle = {Artificial neural networks - ICANN 2009},
 year = {2009},
 address = {Berlin},
 doi = {10.1007/978-3-642-04277-5{\textunderscore }76}
}


@article{BenNasr.2014,
 author = {{Ben Nasr}, Mounir and Chtourou, Mohamed},
 year = {2014},
 title = {Neural network control of nonlinear dynamic systems using hybrid algorithm},
 pages = {423--431},
 volume = {24},
 issn = {15684946},
 journal = {Applied Soft Computing},
 doi = {10.1016/j.asoc.2014.07.023}
}


@book{Bishop.2010,
 author = {Bishop, Christopher M.},
 year = {2010},
 title = {Neural networks for pattern recognition},
 keywords = {Mustererkennung;Neuronales Netz;Statistik},
 address = {Oxford},
 edition = {Reprinted.},
 publisher = {{Oxford Univ. Press}},
 isbn = {9780198538646}
}


@proceedings{Chai.2006,
 year = {2006},
 title = {The Sixth World Congress on Intelligent Control and Automation, 2006, WCICA 2006: 21 - 23 June 2006, [Dalian, China},
 keywords = {Automation;Congresses;Intelligent control systems},
 address = {Piscataway, NJ},
 publisher = {{IEEE Operations Center}},
 isbn = {1-4244-0332-4},
 editor = {Chai, Tianyou},
 institution = {{Da lian li gong da xue} and Dalian-Haishi-Daxue and {IEEE Robotics and Automation Society} and {World Congress on Intelligent Control and Automation} and {WCICA 2006}}
}


@article{Cheng.2009,
 author = {Cheng, Long and Hou, Zeng-Guang and Tan, Min},
 year = {2009},
 title = {Adaptive neural network tracking control for manipulators with uncertain kinematics, dynamics and actuator model},
 pages = {2312--2318},
 volume = {45},
 number = {10},
 issn = {00051098},
 journal = {Automatica},
 doi = {10.1016/j.automatica.2009.06.007}
}


@misc{Dauphin.6102014,
 abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.},
 author = {Dauphin, Yann and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
 date = {6/10/2014},
 title = {Identifying and attacking the saddle point problem in high-dimensional  non-convex optimization},
 url = {http://arxiv.org/pdf/1406.2572v1},
 keywords = {Computer Science - Learning;Mathematics - Optimization and Control;Statistics - Machine Learning}
}


@article{Deng.2008,
 abstract = {A new feedback-linearization-based neural network (NN) adaptive control is proposed for unknown nonaffine nonlinear discrete-time systems. An equivalent model in affine-like form is first derived for the original nonaffine discrete-time systems as feedback linearization methods cannot be implemented for such systems. Then, feedback linearization adaptive control is implemented based on the affine-like equivalent model identified with neural networks. Pretraining is not required and the weights of the neural networks used in adaptive control are directly updated online based on the input-output measurement. The dead-zone technique is used to remove the requirement of persistence excitation during the adaptation. With the proposed neural network adaptive control, stability and performance of the closed-loop system are rigorously established. Illustrated examples are provided to validate the theoretical findings.},
 author = {Deng, Hua and Li, Han-Xiong and Wu, Yi-Hu},
 year = {2008},
 title = {Feedback-linearization-based neural adaptive control for unknown nonaffine nonlinear discrete-time systems},
 keywords = {Algorithms;Artificial intelligence;Computer simulation;Feedback;Linear Models;Neural Networks (Computer);Nonlinear Dynamics;Pattern Recognition, Automated/methods;Signal Processing, Computer-Assisted},
 pages = {1615--1625},
 volume = {19},
 number = {9},
 issn = {1045-9227},
 journal = {IEEE transactions on neural networks},
 doi = {10.1109/TNN.2008.2000804}
}


@misc{Dobel.2018,
 author = {D{\"o}bel, Inga and Leis, Miriam and Vogelsang, Manuel-Molina and Neustroev, Dmitry and Petzka, Henning and R{\"u}ping, Stefan and Voss, Angelika and Mengele, Martin and Welz, Juliane},
 year = {2018},
 title = {Maschinelles Lernen: Kompetenzen, Anwendungen und Forschungsbedarf},
 editor = {Fraunhofer-Gesellschaft}
}


@book{Donahoe.1997,
 abstract = {This internationally authored volume presents major findings, concepts, and methods of behavioral neuroscience coordinated with their simulation via neural networks. A central theme is that biobehaviorally constrained simulations provide a rigorous means to explore the implications of relatively simple processes for the understanding of cognition (complex behavior). Neural networks are held to serve the same function for behavioral neuroscience as population genetics for evolutionary science. The volume is divided into six sections, each of which includes both experimental and simulation research: (1) neurodevelopment and genetic algorithms, (2) synaptic plasticity (LTP), (3) sensory/hippocampal systems, (4) motor systems, (5) plasticity in large neural systems (reinforcement learning), and (6) neural imaging and language. The volume also includes an integrated reference section and a comprehensive index.



This internationally authored volume presents major findings, concepts, and methods of behavioral neuroscience coordinated with their simulation via neural networks. A central theme is that biobehaviorally constrained simulations provide a rigorous means to explore the implications of relatively simple processes for the understanding of cognition (complex behavior). Neural networks are held to serve the same function for behavioral neuroscience as population genetics for evolutionary science. The volume is divided into six sections, each of which includes both experimental and simulation research: (1) neurodevelopment and genetic algorithms, (2) synaptic plasticity (LTP), (3) sensory/hippocampal systems, (4) motor systems, (5) plasticity in large neural systems (reinforcement learning), and (6) neural imaging and language. The volume also includes an integrated reference section and a comprehensive index},
 year = {1997},
 title = {Neural-network models of cognition: Biobehavioral foundations},
 keywords = {Cognitive neuroscience;Neural networks (Neurobiology);Neuropsychology},
 address = {Amsterdam and New York},
 volume = {121},
 publisher = {Elsevier},
 isbn = {9780444819314},
 series = {Advances in Psychology},
 editor = {Donahoe, John W. and Dorsel, Vivian Packard}
}


@book{Duriez.2017,
 abstract = {Preface -- 1 Introduction -- 1.1 Feedback in engineering and living systems -- 1.2 Benefits of feedback control -- 1.3 Challenges of feedback control -- 1.4 Feedback turbulence control is a grand challenge problem -- 1.5 Nature teaches us the control design -- 1.6 Outline of the book -- 1.7 Exercises -- 2 Machine learning control (MLC) -- 2.1 Methods of machine learning -- 2.2 MLC with genetic programming -- 2.3 Examples -- 2.4 Exercises -- 2.5 Suggested reading -- 2.6 Interview with Professor Marc Schoenauer -- 3 Methods of linear control theory -- 3.1 Linear systems -- 3.2 Full-state feedback -- Linear quadratic regulator (LQR) -- 3.3 Sensor-based state estimation -- Kalman filtering -- 3.4 Sensor-based feedback -- Linear quadratic Gaussian (LQG) -- 3.5 System Identification and Model Reduction -- 3.6 Exercises -- 3.7 Suggested reading -- 4 Benchmarking MLC against linear control -- 4.1 Comparison of MLC with LQR on a linear oscillator -- 4.2 Comparison of MLC with Kalman filter on a noisy linear oscillator -- 4.3 Comparison of MLC with LQG for sensor-based feedback -- 4.4 Modifications for small nonlinearity -- 4.5 Exercises -- 4.6 Interview with Professor Shervin Bagheri -- 5 Taming nonlinear dynamics with MLC -- 5.1 Generalized mean-field system -- 5.2 Machine learning control -- 5.3 Derivation outline for the generalized mean-field model -- 5.4 Alternative control approaches -- 5.5 Exercises -- 5.6 Suggested reading -- 5.7 Interview with Professor Mark N. Glauser -- 6 Taming real world flow control experiments with MLC -- 6.1 Separation control over a backward-facing step -- 6.2 Separation control of turbulent boundary layers -- 6.3 Control of mixing layer growth -- 6.4 Alternative model-based control approaches -- 6.5 Implementation of MLC in experiments -- 6.6 Suggested reading -- 6.7 Interview with Professor David Williams -- 7 MLC tactics and strategy -- 7.1 The ideal flow control experiment -- 7.2 Desiderata of the control problem - from the definition to hardware choices -- 7.3 Time scales of MLC -- 7.4 MLC parameters and convergence -- 7.5 The imperfect experiment -- 8 Future developments -- 8.1 Methodological advances of MLC -- 8.2 System-reduction techniques for MLC - Coping with high-dimensional input and output -- 8.3 Future applications of MLC -- 8.4 Exercises -- 8.5 Interview with Professor Belinda Batten -- Glossary -- Symbols -- Abbreviations -- Matlab{\circledR} Code: OpenMLC -- Bibliography -- Index



This is the first book on a generally applicable control strategy for turbulence and other complex nonlinear systems. The approach of the book employs powerful methods of machine learning for optimal nonlinear control laws. This machine learning control (MLC) is motivated and detailed in Chapters 1 and 2. In Chapter 3, methods of linear control theory are reviewed. In Chapter 4, MLC is shown to reproduce known optimal control laws for linear dynamics (LQR, LQG). In Chapter 5, MLC detects and exploits a strongly nonlinear actuation mechanism of a low-dimensional dynamical system when linear control methods are shown to fail. Experimental control demonstrations from a laminar shear-layer to turbulent boundary-layers are reviewed in Chapter 6, followed by general good practices for experiments in Chapter 7. The book concludes with an outlook on the vast future applications of MLC in Chapter 8. Matlab codes are provided for easy reproducibility of the presented results. The book includes interviews with leading researchers in turbulence control (S. Bagheri, B. Batten, M. Glauser, D. Williams) and machine learning (M. Schoenauer) for a broader perspective. All chapters have exercises and supplemental videos will be available through YouTube},
 author = {Duriez, Thomas and Brunton, Steven L. and Noack, Bernd R.},
 year = {2017},
 title = {Machine Learning Control - Taming Nonlinear Dynamics and Turbulence},
 url = {http://dx.doi.org/10.1007/978-3-319-40624-4},
 address = {Cham and s.l.},
 volume = {116},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-319-40624-4},
 series = {Fluid Mechanics and Its Applications},
 doi = {10.1007/978-3-319-40624-4}
}


@article{Elman.1990,
 author = {Elman, Jeffrey L.},
 year = {1990},
 title = {Finding Structure in Time},
 pages = {179--211},
 volume = {14},
 number = {2},
 issn = {03640213},
 journal = {Cognitive Science},
 doi = {10.1207/s15516709cog1402{\textunderscore }1}
}


@inproceedings{Fumumoto.2017,
 author = {Fumumoto, Yuta and Owaki, Shotaro and Nakamura, Moriya},
 title = {Effect of number of neurons of a neural-network on compensation performance of SPM non-linear waveform distortion},
 pages = {1--2},
 publisher = {IEEE},
 isbn = {978-1-5090-6293-5},
 booktitle = {2017 Opto-Electronics and Communications Conference (OECC) and Photonics Global Conference (PGC)},
 year = {2017},
 address = {Piscataway, NJ},
 doi = {10.1109/OECC.2017.8114880}
}


@article{Gerwin.1993,
 abstract = {To help meet competitive realities operations managers need to know more about the strategic aspects of manufacturing flexibility. This paper takes steps toward meeting that need by critically reviewing the literature and establishing a research agenda for the area. A conceptual model, which places flexibility within a broad context, helps to identify certain assumptions of theoretical studies which need to be challenged. The model also provides a basis for identifying specific flexibility dimensions. The manner in which these dimensions may limit the effectiveness of a manufacturing process, and the problems in operationalizing them are discussed. Focusing next on the neglected area of applied work, concepts are presented for analyzing whether desired amounts of flexibility are being achieved and whether the potential for flexibility built into a manufacturing process is being tapped. Once more, a procedure is outlined for altering a plant's types and amounts of flexibility over time. The research agenda, which grows out of the appraisal of theoretical and applied work, indicates the value in studying generic flexibility strategies, the flexibility dimensions, methods of delivery, ways of evaluating and changing a process's flexibility, and above all measurement problems. The conclusions indicate principles for strategic research, some of which have relevance for the development of mathematical models.},
 author = {Gerwin, Donald},
 year = {1993},
 title = {Manufacturing Flexibility: A Strategic Perspective},
 url = {http://www.jstor.org/stable/2632407},
 pages = {395--410},
 volume = {39},
 number = {4},
 issn = {00251909},
 journal = {Management Science}
}


@article{Groche.2004,
 author = {Groche, P. and Schneider, R.},
 year = {2004},
 title = {Method for the Optimization of Forming Presses for the Manufacturing of Micro Parts},
 pages = {281--284},
 volume = {53},
 number = {1},
 issn = {00078506},
 journal = {CIRP Annals},
 doi = {10.1016/S0007-8506(07)60698-2}
}


@article{Groche.2010,
 author = {Groche, P. and Scheitza, M. and Kraft, M. and Schmitt, S.},
 year = {2010},
 title = {Increased total flexibility by 3D Servo Presses},
 pages = {267--270},
 volume = {59},
 number = {1},
 issn = {00078506},
 journal = {CIRP Annals},
 doi = {10.1016/j.cirp.2010.03.013}
}


@article{Han.2009,
 author = {Han, S. I. and Lee, K. S.},
 year = {2009},
 title = {Sliding mode-based friction control with adaptive dual friction observer and intelligent uncertainty compensator},
 pages = {1129--1147},
 volume = {223},
 number = {8},
 issn = {0959-6518},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part I: Journal of Systems and Control Engineering},
 doi = {10.1243/09596518JSCE811}
}


@misc{Han.682015,
 abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.},
 author = {Han, Song and Pool, Jeff and Tran, John and Dally, William J.},
 date = {6/8/2015},
 title = {Learning both Weights and Connections for Efficient Neural Networks},
 url = {http://arxiv.org/pdf/1506.02626v3},
 keywords = {Computer Science - Computer Vision and Pattern Recognition;Computer Science - Learning;Computer Science - Neural and Evolutionary Computing}
}


@article{Hassoun.1996,
 author = {Hassoun, M. H.},
 year = {1996},
 title = {Fundamentals of Artificial Neural Networks},
 pages = {906},
 volume = {84},
 number = {6},
 issn = {0018-9219},
 journal = {Proceedings of the IEEE},
 doi = {10.1109/JPROC.1996.503146}
}


@incollection{Hochreiter.2001,
 author = {Hochreiter, Sepp and Bengio, Yoshua and Frasconi, Paolo},
 title = {Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies},
 publisher = {{IEEE Press}},
 editor = {Kolen, J. and Kremer, S.},
 booktitle = {Field Guide to Dynamical Recurrent Networks},
 year = {2001}
}


@article{Hong.2009,
 author = {Hong, Chun-Han and Choi, Kwang-Chan and Kim, Byoung-Soo},
 year = {2009},
 title = {Applications of adaptive neural network control to an unmanned airship},
 pages = {911--917},
 volume = {7},
 number = {6},
 issn = {1598-6446},
 journal = {International Journal of Control, Automation and Systems},
 doi = {10.1007/s12555-009-0606-9}
}


@article{Hornik.1989,
 author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
 year = {1989},
 title = {Multilayer feedforward networks are universal approximators},
 pages = {359--366},
 volume = {2},
 number = {5},
 issn = {08936080},
 journal = {Neural Networks},
 doi = {10.1016/0893-6080(89)90020-8}
}


@article{Hsu.2009,
 author = {Hsu, Chun-Fei and Cheng, Kuo-Hsiang and Lee, Tsu-Tian},
 year = {2009},
 title = {Robust wavelet-based adaptive neural controller design with a fuzzy compensator},
 pages = {423--431},
 volume = {73},
 number = {1-3},
 issn = {09252312},
 journal = {Neurocomputing},
 doi = {10.1016/j.neucom.2009.07.011}
}


@article{Hunt.1991,
 author = {Hunt, K. J. and Sbarbaro, D.},
 year = {1991},
 title = {Neural networks for nonlinear internal model control},
 pages = {431},
 volume = {138},
 number = {5},
 issn = {01437054},
 journal = {IEE Proceedings D Control Theory and Applications},
 doi = {10.1049/ip-d.1991.0059}
}


@article{HUSSAIN.1999,
 author = {HUSSAIN, MOHAMED AZLAN and KERSHENBAUM, L. S.},
 year = {1999},
 title = {SIMULATION AND EXPERIMENTAL IMPLEMENTATION OF A NEURAL-NETWORK-BASED INTERNAL-MODEL CONTROL STRATEGY ON A REACTOR SYSTEM},
 pages = {151--169},
 volume = {172},
 number = {1},
 issn = {0098-6445},
 journal = {Chemical Engineering Communications},
 doi = {10.1080/00986449908912768}
}


@proceedings{IEEENeuralNetworksCouncilStaff.June1992,
 year = {June 1992},
 title = {IEEE-INNS International Joint Conference on Neural Networks, Baltimore, 1992},
 address = {Piscataway},
 publisher = {IEEE},
 isbn = {0-7803-0559-0},
 institution = {{IEEE, Neural Networks Council Staff} and {IEEE, Society Staff}}
}


@proceedings{IEEENeuralNetworksCouncilStaff.March1993,
 year = {March 1993},
 title = {IEEE International Conference on Neural Networks 1993},
 address = {Piscataway},
 publisher = {IEEE},
 isbn = {0-7803-0999-5},
 institution = {{IEEE, Neural Networks Council Staff} and {IEEE, Institute of Electrical and Electronics Engineers, Inc. Staff}}
}


@proceedings{InstituteofElectricalandElectronicsEngineers.2009,
 year = {2009},
 title = {International Conference on Information and Automation, 2009: ICIA '09 ; 22 - 24 June 2009, Zhuhai/Macau, China},
 keywords = {Artificial intelligence;Automation;Congresses;Control systems;Robots},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4244-3607-1},
 institution = {{Institute of Electrical and Electronics Engineers} and {IEEE International Conference on Information and Automation} and ICIA}
}


@article{Isermann.2010,
 author = {Isermann, Rolf},
 year = {2010},
 title = {Modellbasierte {\"U}berwachung und Fehlerdiagnose von kontinuierlichen technischen Prozessen},
 volume = {58},
 number = {6},
 issn = {0178-2312},
 journal = {at - Automatisierungstechnik},
 doi = {10.1524/auto.2010.0846}
}


@book{Isermann.2011,
 author = {Isermann, Rolf and M{\"u}nchhof, Marco},
 year = {2011},
 title = {Identification of Dynamic Systems: An Introduction with Applications},
 url = {http://dx.doi.org/10.1007/978-3-540-78879-9},
 address = {Berlin, Heidelberg},
 publisher = {{Springer-Verlag Berlin Heidelberg}},
 isbn = {9783540788782},
 series = {Advanced Textbooks in Control and Signal Processing},
 doi = {10.1007/978-3-540-78879-9}
}


@misc{JohnDuchi.2010,
 author = {Duchi, John and Hazan, Elad and Singer, Yoram},
 year = {2010},
 title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}
}


@article{Jolly.2009,
 author = {Jolly, K. G. and {Sreerama Kumar}, R. and Vijayakumar, R.},
 year = {2009},
 title = {An artificial neural network based dynamic controller for a robot in a multi-agent system},
 pages = {283--294},
 volume = {73},
 number = {1-3},
 issn = {09252312},
 journal = {Neurocomputing},
 doi = {10.1016/j.neucom.2009.08.011}
}


@incollection{Jordan.1997,
 author = {Jordan, Michael I.},
 title = {Serial Order: A Parallel Distributed Processing Approach},
 pages = {471--495},
 volume = {121},
 publisher = {Elsevier},
 isbn = {9780444819314},
 series = {Advances in Psychology},
 editor = {Donahoe, John W. and Dorsel, Vivian Packard},
 booktitle = {Neural-network models of cognition},
 year = {1997},
 address = {Amsterdam and New York},
 doi = {10.1016/S0166-4115(97)80111-2}
}


@inproceedings{Jung.1995,
 author = {Jung, Seul and Hsia, T. C.},
 title = {A new neural network control technique for robot manipulators},
 pages = {878--882},
 publisher = {{Distributed through the IEEE Service Center}},
 isbn = {0-7803-2445-5},
 booktitle = {Proceedings of the 1995 American Control Conference},
 year = {1995},
 address = {Piscataway, N.J},
 doi = {10.1109/ACC.1995.529374}
}


@inproceedings{Kalman.June1992,
 author = {Kalman, B. L. and Kwasny, S. C.},
 title = {Why tanh: choosing a sigmoidal function},
 pages = {578--581},
 publisher = {IEEE},
 isbn = {0-7803-0559-0},
 booktitle = {IEEE-INNS International Joint Conference on Neural Networks, Baltimore, 1992},
 year = {June 1992},
 address = {Piscataway},
 doi = {10.1109/IJCNN.1992.227257}
}


@article{Kambhampati.2000,
 author = {Kambhampati, C. and Craddock, R. J. and Tham, M. and Warwick, K.},
 year = {2000},
 title = {Inverse model control using recurrent networks},
 pages = {181--199},
 volume = {51},
 number = {3-4},
 issn = {03784754},
 journal = {Mathematics and Computers in Simulation},
 doi = {10.1016/S0378-4754(99)00116-0}
}


@article{KKaramodin.2009,
 author = {K-Karamodin, A. and H-Kazemi, H.},
 year = {2009},
 title = {Semi-active control of structures using neuro-predictive algorithm for MR dampers},
 pages = {n/a-n/a},
 volume = {5},
 number = {4},
 issn = {15452255},
 journal = {Structural Control and Health Monitoring},
 doi = {10.1002/stc.278}
}


@article{KOKER.2006,
 author = {KOKER, R.},
 year = {2006},
 title = {Design and performance of an intelligent predictive controller for a six-degree-of-freedom robot using the Elman network},
 pages = {1781--1799},
 volume = {176},
 number = {12},
 issn = {00200255},
 journal = {Information Sciences},
 doi = {10.1016/j.ins.2005.05.002}
}


@book{Kolen.2001,
 author = {Kolen, John F. and Kremer, Stefan C.},
 year = {2001},
 title = {A field guide to dynamical recurrent networks},
 keywords = {Neural networks (Computer science)},
 address = {New York},
 publisher = {{IEEE Press}},
 isbn = {9780470544037}
}


@book{Kolen.2001b,
 year = {2001},
 title = {Field Guide to Dynamical Recurrent Networks},
 publisher = {{IEEE Press}},
 editor = {Kolen, J. and Kremer, S.}
}


@book{Kolen.2009,
 abstract = {Preface. Acknowledgments. List of Figures. List of Tables. List of Contributors. INTRODUCTION. Dynamical Recurrent Networks (J. Kolen and S. Kremer). ARCHITECTURES. Networks with Adaptive State Transitions (D. Calvert and S. Kremer). Delay Networks: Buffers to Rescue (T. Lin and C. Giles). Memory Kernels (A. Tsoi, et al.). CAPABILITIES. Dynamical Systems and Iterated Function Systems (J. Kolen). Representation of Discrete States (C. Giles and C. Omlin). Simple Stable Encodings of Finite-State Machines in Dynamic Recurrent Networks (M. Forcada and R. Carrasco). Representation Beyond Finite States: Alternatives to Pushdown Automata (J. Wiles, et al.). Universal Computation and Super-Turing Capabilities (H. Siegelmann). ALGORITHMS. Insertion of Prior Knowledge (P. Frasconi, et al.). Gradient Calculations for Dynamic Recurrent Neural Networks (B. Pearlmutter). Understanding and Explaining DRN Behavior (C. Omlin). LIMITATIONS. Evaluating Benchmark Problems by Random Guessing (J. Schmidhuber, et al.). Gradient Flow in Recurrent Nets: The Difficulty of Learning Long-Term Dependencies (S. Hochreiter, et al.. Limiting the Computational Power of Recurrent Neural Networks: VC Dimension and Noise (C. Moore). APPLICATIONS. Dynamical Reccurent Networks in Control (D. Prokhorov, et al.). Sentence Processing and Linguistic Structure (W. Tabor). Neural Network Architectures for the Modeling of Dynamic Systems (H. Zimmerman and R. Neuneier). From Sequences to Data Structures: Theory and Applications (P. Frasconi, et al.). CONCLUSION. Dynamical Recurrent Networks: Looking Back and Looking Forward (S. Kremer and J. Kolen). Bibliography. Glossary. Index. About the Editors


Acquire the tools for understanding new architectures and algorithms of dynamical recurrent networks (DRNs) from this valuable field guide, which documents recent forays into artificial intelligence, control theory, and connectionism. This unbiased introduction to DRNs and their application to time-series problems (such as classification and prediction) provides a comprehensive overview of the recent explosion of leading research in this prolific field. A Field Guide to Dynamical Recurrent Networks emphasizes the issues driving the development of this class of network structures. It provides a solid foundation in DRN systems theory and practice using consistent notation and terminology. Theoretical presentations are supplemented with applications ranging from cognitive modeling to financial forecasting. A Field Guide to Dynamical Recurrent Networks will enable engineers, research scientists, academics, and graduate students to apply DRNs to various real-world problems and learn about different areas of active research. It provides both state-of-the-art information and a road map to the future of cutting-edge dynamical recurrent networks},
 year = {2009},
 title = {A field guide to dynamical recurrent networks},
 keywords = {Neural networks (Computer science)},
 address = {New York and Piscataway, New Jersey},
 publisher = {{IEEE Press} and {IEEE Xplore}},
 isbn = {9780780353695},
 editor = {Kolen, John F. and Kremer, Stefan C.},
 doi = {10.1109/9780470544037}
}


@inproceedings{Kollias.March1993,
 author = {Anastassiou and Kollias},
 title = {Adaptive training of multilayer neural networks using a least squares estimation technique},
 pages = {383-390 vol.1},
 publisher = {IEEE},
 isbn = {0-7803-0999-5},
 booktitle = {IEEE International Conference on Neural Networks 1993},
 year = {March 1993},
 address = {Piscataway},
 doi = {10.1109/ICNN.1988.23870}
}


@article{Leeghim.2008,
 author = {Leeghim, Henzeh and Seo, In-Ho and Bang, Hyochoong},
 year = {2008},
 title = {Adaptive nonlinear control using input normalized neural networks},
 pages = {1073--1083},
 volume = {22},
 number = {6},
 issn = {1738-494X},
 journal = {Journal of Mechanical Science and Technology},
 doi = {10.1007/s12206-007-1119-1}
}


@inproceedings{Li.2009,
 author = {Li, Hongxing and Zhang, Yinong and Wu, Xuetao},
 title = {Nonlinear internal model control using neural networks for gas collectors of coke oven},
 pages = {1177--1182},
 publisher = {IEEE},
 isbn = {978-1-4244-3607-1},
 booktitle = {International Conference on Information and Automation, 2009},
 year = {2009},
 address = {Piscataway, NJ},
 doi = {10.1109/ICINFA.2009.5205095}
}


@misc{Lipton.5292015,
 abstract = {Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities. Recurrent neural networks (RNNs) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window. Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them. In recent years, systems based on long short-term memory (LSTM) and bidirectional (BRNN) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a self-contained explication of the state of the art together with a historical perspective and references to primary research.},
 author = {Lipton, Zachary C. and Berkowitz, John and Elkan, Charles},
 date = {5/29/2015},
 title = {A Critical Review of Recurrent Neural Networks for Sequence Learning},
 url = {http://arxiv.org/pdf/1506.00019v4},
 keywords = {Computer Science - Learning;Computer Science - Neural and Evolutionary Computing}
}


@inproceedings{Liu.2006,
 author = {Liu, Xiang-Jie and Liu, Ji-Zhen},
 title = {Neuro-Fuzzy Generalized Predictive Control of Boiler Steam Temperature},
 pages = {6531--6535},
 publisher = {{IEEE Operations Center}},
 isbn = {1-4244-0332-4},
 editor = {Chai, Tianyou},
 booktitle = {The Sixth World Congress on Intelligent Control and Automation, 2006, WCICA 2006},
 year = {2006},
 address = {Piscataway, NJ},
 doi = {10.1109/WCICA.2006.1714344}
}


@book{Lunze.2013,
 abstract = {Dieses Lehrbuch~{\"u}berzeugt durch seine Didaktik und Stoffauswahl. Die Darstellung zielt auf ein tiefgr{\"u}ndiges Verst{\"a}ndnis dynamischer Systeme und Regelungsvorg{\"a}nge, wobei mit Zeitbereichsbetrachtungen im Zustandsraum begonnen und erst danach zur Frequenzbereichsdarstellung {\"u}bergegangen wird. Praktische Beispiele aus Elektrotechnik, Maschinenbau, Verfahrenstechnik und Verkehrstechnik illustrieren die Anwendung der behandelten Methoden und zeigen den fach{\"u}bergreifenden Charakter der Regelungstechnik. Mit der Einf{\"u}hrung in MATLAB (Release R2012a) wird der Anschluss an die rechnergest{\"u}tzte Arbeitsweise der Ingenieure hergestellt. {\"U}bungsaufgaben mit ausf{\"u}hrlichen L{\"o}sungen dienen der Vertiefung des Stoffes. F{\"u}r die neunte Auflage wurden zahlreiche Abbildungen verbessert und die Beschreibung von MATLAB der aktuellen Version angepasst.{\glqq}Das Buch vermittelt in idealer Weise theoretisch begr{\"u}ndetes Verst{\"a}ndnis mit praktischen Gesichtspunkten und Beispielen aus verschiedenen Bereichen.{\grqq} Prof. Dr.-Ing. habil. Dr. h.c. Bernhard Lampe, Universit{\"a}t Rostock {\glqq}Das Buch wird von meinen Studenten und Doktoranden sehr gesch{\"a}tzt, weil es zum einen den Grundlagenstoff klar und vollst{\"a}ndig bringt, zum anderen weiterf{\"u}hrende Themen und Prinzipien in knapper und verst{\"a}ndlicher Form erg{\"a}nzt.{\grqq} Prof. Dr.-Ing. Boris Lohmann, Technische Universit{\"a}t M{\"u}nchen},
 author = {Lunze, Jan},
 year = {2013},
 title = {Regelungstechnik 1: Systemtheoretische Grundlagen, Analyse und Entwurf einschleifiger Regelungen},
 url = {http://dx.doi.org/10.1007/978-3-642-29533-1},
 address = {Berlin},
 edition = {9., {\"u}berarb. Aufl.},
 publisher = {Springer},
 isbn = {9783642295324},
 series = {Springer-Lehrbuch},
 doi = {10.1007/978-3-642-29533-1}
}


@book{Maren.2014,
 abstract = {Handbook of Neural Computing Applications



Handbook of Neural Computing Applications},
 year = {2014},
 title = {Handbook of Neural Computing Applications},
 address = {Burlington},
 publisher = {{Elsevier Science}},
 isbn = {9780125460903},
 editor = {Maren, Alianna J. and Harston, Craig T. and Pap, Robert M.}
}


@book{Masters.1995,
 author = {Masters, Timothy},
 year = {1995},
 title = {Advanced algorithms for neural networks A C++ sourcebook},
 url = {http://www.loc.gov/catdir/enhancements/fy0706/94043390-b.html},
 keywords = {Algorithmus;C {\textless}Programmiersprache{\textgreater};C++ (Computer program language);Computer algorithms;Neural networks (Computer science);Neuronales Netz;Neuronales Netz / Algorithmus / C++ / Programm},
 address = {New York},
 edition = {1. Dr},
 publisher = {Wiley},
 isbn = {0471105880}
}


@article{Miller.1989,
 author = {Miller, W. T.},
 year = {1989},
 title = {Real-time application of neural networks for sensor-based control of robots with vision},
 pages = {825--831},
 volume = {19},
 number = {4},
 issn = {00189472},
 journal = {IEEE Transactions on Systems, Man, and Cybernetics},
 doi = {10.1109/21.35345}
}


@article{Mohammadzaheri.2010,
 author = {Mohammadzaheri, Morteza and Chen, Lei},
 year = {2010},
 title = {Intelligent predictive control of a model helicopter's yaw angle},
 pages = {667--679},
 volume = {12},
 number = {6},
 issn = {15618625},
 journal = {Asian Journal of Control},
 doi = {10.1002/asjc.243}
}


@article{Mohammadzaheri.2012,
 author = {Mohammadzaheri, Morteza and Chen, Lei and Grainger, Steven},
 year = {2012},
 title = {A critical review of the most popular types of neuro control},
 pages = {1--11},
 volume = {14},
 number = {1},
 issn = {15618625},
 journal = {Asian Journal of Control},
 doi = {10.1002/asjc.449}
}


@article{Nguyen.1990,
 author = {Nguyen, D. H. and Widrow, B.},
 year = {1990},
 title = {Neural networks for self-learning control systems},
 pages = {18--23},
 volume = {10},
 number = {3},
 issn = {0272-1708},
 journal = {IEEE Control Systems Magazine},
 doi = {10.1109/37.55119}
}


@article{Nordgren.1993,
 abstract = {A neural network inverse dynamics controller with adjustable weights is compared with a computed-torque type adaptive controller. Lyapunov stability techniques, usually applied to adaptive systems, are used to derive a globally asymptotically stable adaptation law for a single-layer neural network controller that bears similarities to the well-known delta rule for neural networks. This alternative learning rule allows the learning rates of each connection weight to be individually adjusted to give faster convergence. The role of persistently exciting inputs in ensuring parameter convergence, often mentioned in the context of adaptive systems, is emphasized in relation to the convergence of neural network weights. A coupled, compound pendulum system is used to develop inverse dynamics controllers based on adaptive and neural network techniques. Adaptation performance is compared for a model-based adaptive controller and a simple neural network utilizing both delta-rule learning and the alternative adaptation law.},
 author = {Nordgren, R. E. and Meckl, P. H.},
 year = {1993},
 title = {An analytical comparison of a neural network and a model-based adaptive controller},
 pages = {685--694},
 volume = {4},
 number = {4},
 issn = {1045-9227},
 journal = {IEEE transactions on neural networks},
 doi = {10.1109/72.238322}
}


@proceedings{OECC.2017,
 year = {2017},
 title = {2017 Opto-Electronics and Communications Conference (OECC) and Photonics Global Conference (PGC)},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-5090-6293-5},
 institution = {OECC and {Photonics Global Conference} and {IEEE Photonics Society} and {Optical Society of America} and {Opto-Electronics and Communications Conference} and PGC}
}


@article{Parekh.2000,
 abstract = {Constructive learning algorithms offer an attractive approach for the incremental construction of near-minimal neural-network architectures for pattern classification. They help overcome the need for ad hoc and often inappropriate choices of network topology in algorithms that search for suitable weights in a priori fixed network architectures. Several such algorithms are proposed in the literature and shown to converge to zero classification errors (under certain assumptions) on tasks that involve learning a binary to binary mapping (i.e., classification problems involving binary-valued input attributes and two output categories). We present two constructive learning algorithms MPyramid-real and MTiling-real that extend the pyramid and tiling algorithms, respectively, for learning real to M-ary mappings (i.e., classification problems involving real-valued input attributes and multiple output classes). We prove the convergence of these algorithms and empirically demonstrate their applicability to practical pattern classification problems. Additionally, we show how the incorporation of a local pruning step can eliminate several redundant neurons from MTiling-real networks.},
 author = {Parekh, R. and Yang, J. and Honavar, V.},
 year = {2000},
 title = {Constructive neural-network learning algorithms for pattern classification},
 pages = {436--451},
 volume = {11},
 number = {2},
 issn = {1045-9227},
 journal = {IEEE transactions on neural networks},
 doi = {10.1109/72.839013}
}


@article{Parlos.2001,
 author = {Parlos, A. G. and Parthasarathy, Sanjay and Atiya, A. F.},
 year = {2001},
 title = {Neuro-predictive process control using online controller adaptation},
 pages = {741--755},
 volume = {9},
 number = {5},
 issn = {10636536},
 journal = {IEEE Transactions on Control Systems Technology},
 doi = {10.1109/87.944469}
}


@misc{Pascanu.11212012,
 abstract = {There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
 author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
 date = {11/21/2012},
 title = {On the difficulty of training Recurrent Neural Networks},
 url = {http://arxiv.org/pdf/1211.5063v2},
 keywords = {Computer Science - Learning}
}


@inproceedings{Piovoso.1991,
 author = {Piovoso, Michael J. and Owens, Aaron J.},
 title = {Neural network process control},
 pages = {84--94},
 publisher = {ACM},
 isbn = {0897914325},
 editor = {Barrett, Larry K.},
 booktitle = {Proceedings of the conference on Analysis of neural network applications},
 year = {1991},
 address = {New York, NY},
 doi = {10.1145/106965.105256}
}


@incollection{Piovoso.1991b,
 author = {Piovoso, Michael J. and Owens, Aaron J.},
 title = {Sensor Data Analysis Using Neural Networks},
 publisher = {CACHE},
 isbn = {0816905495},
 series = {AIChE publication},
 editor = {Arkun, Yaman and Ray, Willis Harmon},
 booktitle = {Chemical process control-CPCIV},
 year = {1991},
 address = {Austin Tex.}
}


@article{Prasad.1998,
 author = {Prasad, G. and Swidenbank, E. and Hogg, B. W.},
 year = {1998},
 title = {A neural net model-based multivariable long-range predictive control strategy applied in thermal power plant control},
 pages = {176--182},
 volume = {13},
 number = {2},
 issn = {08858969},
 journal = {IEEE Transactions on Energy Conversion},
 doi = {10.1109/60.678982}
}


@article{Prasad.2014,
 author = {Prasad, Lal Bahadur and Gupta, Hari Om and Tyagi, Barjeev},
 year = {2014},
 title = {Application of policy iteration technique based adaptive optimal control design for automatic voltage regulator of power system},
 pages = {940--949},
 volume = {63},
 issn = {01420615},
 journal = {International Journal of Electrical Power {\&} Energy Systems},
 doi = {10.1016/j.ijepes.2014.06.057}
}


@book{Rakowitsch.2018,
 author = {Rakowitsch, Martin},
 year = {2018},
 title = {Modellierung der Dynamik eines Koppelgetriebes am Beispiel der 3D-Servo-Presse: Studienarbeit},
 address = {Darmstadt}
}


@article{Ren.2009,
 author = {Ren, X. M. and Rad, A. B.},
 year = {2009},
 title = {Adaptive non-linear compensation control based on neural networks for non-linear systems with time delay},
 pages = {1283--1292},
 volume = {40},
 number = {12},
 issn = {0020-7721},
 journal = {International Journal of Systems Science},
 doi = {10.1080/00207720903038150}
}


@article{Ruan.2007,
 author = {Ruan, Xiaogang and Ding, Mingxiao and Gong, Daoxiong and Qiao, Junfei},
 year = {2007},
 title = {On-line adaptive control for inverted pendulum balancing based on feedback-error-learning},
 pages = {770--776},
 volume = {70},
 number = {4-6},
 issn = {09252312},
 journal = {Neurocomputing},
 doi = {10.1016/j.neucom.2006.10.012}
}


@article{Schmoeckel.1991,
 author = {Schmoeckel, Dieter},
 year = {1991},
 title = {Developments in Automation, Flexibilization and Control of Forming Machinery},
 pages = {615--622},
 volume = {40},
 number = {2},
 issn = {00078506},
 journal = {CIRP Annals},
 doi = {10.1016/S0007-8506(07)61137-8}
}


@book{Schroder.2010,
 abstract = {{\textquotedbl}Das Buch {\textquotedbl}{\textquotedbl}Intelligente Verfahren{\textquotedbl}{\textquotedbl} f{\"u}hrt anschaulich und verst{\"a}ndlich in das Gebiet der intelligenten Verfahren ein. In geschlossenen Darstellungen zeigt das Buch zuerst die theoretischen Grundlagen und anschlie{\ss}end praktische Beispiele, wobei bei den Beispielen auch fehlerhafte Ans{\"a}tze gezeigt werden. Nach einer kurzen {\"U}bersicht werden im zweiten, einf{\"u}hrenden Kapitel die Einschr{\"a}nkungen behandelt, die bereits bei relativ einfachen Systemen zu beachten sind, wenn ein lineares System vorausgesetzt wird. Es wird weiterhin gezeigt, dass praktisch alle Systeme nicht- linear sind und damit die Basis f{\"u}r die kommenden Kapitel gelegt. In vielen Anwendungsf{\"a}llen, in denen die Nichtlinearit{\"a}ten separiert werden k{\"o}nnen. Die Abbildung solcher Nichtlinearit{\"a}ten durch Funktionsapproximatoren wird mit Beispielen im dritten Kapitel abgehandelt. Das vierte Kapitel stellt die grundlegenden Lernverfahren wie der Gradientenabstieg vor, und im f{\"u}nften Kapitel wird das erste intelligente Verfahren, der lernf{\"a}hige, nichtlineare Beobachter entwickelt, wobei angenommen wird, dass nur die Nichtlinearit{\"a}ten wie Reibung und Lose unbekannt sind und deshalb gelernt werden m{\"u}ssen. Im sechsten Kapitel werden die Voraussetzungen der Vorkenntnisse verringert, denn nun ist nur noch die regelungs- technische Struktur des nichtlinearen Systems bekannt. Kapitel sieben und acht stellt allgemein die Identifikationsstrukturen sowie die Vor- und Nachteile bei linearen und nicht- linearen Systemen dar. In Kapitel neun werden Identifikationsverfahren f{\"u}r dynamische Nichtlinearit{\"a}ten vorgestellt. Im zehnten Kapitel erfolgt eine umfassende und kritische Darstellung der h{\"o}herwertigen Lernverfahren. Es wird au{\ss}erdem ein intelligentes Verfahren vorgestellt bei dem Vorwissen eingebracht werden kann. In einem weiteren Kapitel werden die globalen Optimierungsverfahren vorgestellt. Nachdem in den bisherigen Kapiteln die Identifikation nichtlinearer Systeme der Schwerpunkt war, folgen nun, beginnend mit Kapitel elf, die Regelungsverfahren bei nichtlinearen Systemen, wobei auch Randgebiete wie die Bedeutung der Nulldynamik ausf{\"u}hrlich vorgestellt werden. Kapitel dreizehn behandelt u.a. Regelungsverfahren bei denen das nichtlineare System gest{\"o}rt ist bzw. sich in der Struktur oder/ und den Parametern sprungf{\"o}rmig {\"a}ndert. Kapitel vierzehn erweitert den Aspekt des vorherigen Kapitels auf stochastische Prozesse. In Kapitel f{\"u}nfzehn werden Regelungsverfahren f{\"u}r nichtlineare, unbekannte Systeme erarbeitet, die au{\ss}erdem unbekannt gest{\"o}rt sind. Diese Verfahren n{\"u}tzen einen entscheidend ver{\"a}nderten High-Gain-Ansatz. Kapitel sechszehn stellt die Fuzzy-Logik vor. Die Verfahren k{\"o}nnen bei allen nichtlinearen Systemen angewendet werden und wurden bereits {\"a}u{\ss}erst erfolgreich in der Medizin, in der Robotik, in der Getriebesteuerung von Hybridfahrzeugen etc. eingesetzt.{\textquotedbl}



Das Buch f{\"u}hrt anschaulich und verst{\"a}ndlich in das Gebiet der intelligenten Verfahren ein. Nach den theoretischen Grundlagen erl{\"a}utert der Autor praktische Beispiele, wobei er auch auf fehlerhafte Ans{\"a}tze hinweist. In dem Band werden verschiedene Verfahren vorgestellt und intelligente Verfahren entwickelt. Diese wurden bereits erfolgreich in der Medizin, in der Robotik sowie u. a. in der Getriebesteuerung von Hybridfahrzeugen eingesetzt.},
 author = {Schr{\"o}der, Dierk},
 year = {2010},
 title = {Intelligente Verfahren: Identifikation und Regelung nichtlinearer Systeme},
 url = {http://dx.doi.org/10.1007/978-3-642-11398-7},
 address = {Berlin, Heidelberg},
 publisher = {{Springer-Verlag Berlin Heidelberg}},
 isbn = {9783642113987},
 doi = {10.1007/978-3-642-11398-7}
}


@misc{Sinz.2018,
 author = {Sinz, Julian},
 editor = {{Technische Universit{\"a}t Darmstadt  - Institut f{\"u}r Produktionstechnik und Umformmaschinen}},
 year = {2018},
 title = {Die 3D-Servo-Presse - von der Forschungsversion zur industriellen Standardmaschine},
 url = {https://www.ptu.tu-darmstadt.de/mu_forschung/mu_prozesskettenundanlagen/abgeschlossene_forschungsprojekte_pa/die_3d_servo_presse___von_der_forschungsversion_zur_industriellen_standartmaschine/index.de.jsp},
 address = {Darmstadt},
 urldate = {13.09.2018}
}


@book{Sklyarenko.2002,
 author = {Sklyarenko, Yevgen},
 year = {2002},
 title = {Anwendung neuronmaler Netze zur Gegelung von nichtlinearen Roboterantrieben: Dissertation}
}


@article{Son.1987,
 author = {Son, Young Kyu and Park, Chan S.},
 year = {1987},
 title = {Economic measure of productivity, quality and flexibility in advanced manufacturing systems},
 pages = {193--207},
 volume = {6},
 number = {3},
 issn = {02786125},
 journal = {Journal of Manufacturing Systems},
 doi = {10.1016/0278-6125(87)90018-5}
}


@article{StevenA.Harp.1992,
 abstract = {ABSTRACT

We describe an approach to application-specific neural network

design using genetic algorithms. A genetic algorithm is a robust

optimization method particularly well suited for search spaces that are

high-dimensional, discontinuous and noisy-features that typify the neural

network design problem. Our approach is relevant to virtually all neural

network applications: it is network-model independent and it permits

optimization for arbitrary, user-defined criteria. We have developed an

experimental system, NeuroGENESYS, and have conducted several experiments

on small-scale problems. Performance improvements over manual

designs have been observed, the interplay between performance

criteria and network design aspects has been demonstrated, and

general design principles have been uncovered.},
 author = {Harp, Steven A. and Samad, Tariq},
 year = {1992},
 title = {Optimizing Neural Networks with Genetic Algorithms},
 pages = {1138--1143},
 volume = {54 pt 2},
 journal = {Proceedings of the American Power Conference}
}


@book{Sturm.2000,
 author = {Sturm, Michael},
 year = {2000},
 title = {Neuronale Netze zur Modellbildung in der Regelungstechnik: Dissertation}
}


@misc{VDMA.2018,
 author = {Anderl, Reiner and Picard, Andr{\'e} and Wang, Y{\"u}bo and Dosch, Steffen and Klee, Benedikt and Bauer, J{\"o}rg},
 editor = {{VDMA Verlag GmbH}},
 year = {2018},
 title = {Leitfaden Industrie 4.0: Orientierungshilfe zur Einf{\"u}hrung in den Mittelstand},
 url = {https://industrie40.vdma.org/viewer/-/v2article/render/15540546},
 address = {Frankfurt am Main},
 urldate = {14.09.2018}
}


@misc{Velickovic.2018,
 author = {Veli{\v{c}}kovi{\'c}, Petar},
 year = {2018},
 title = {: Multilayer perceptron~(MLP): A diagram representing an in-depth view at a single perceptron, along with its position within a larger-scale multilayer perceptron ({\~{}}unrestricted feedforward neural network).},
 url = {https://github.com/PetarV-/TikZ/tree/master/Multilayer%20perceptron},
 urldate = {21.09.2018}
}


@article{VijayaKumar.2009,
 author = {{Vijaya Kumar}, M. and Suresh, S. and Omkar, S. N. and Ganguli, Ranjan and Sampath, Prasad},
 year = {2009},
 title = {A direct adaptive neural command controller design for an unstable helicopter},
 pages = {181--191},
 volume = {22},
 number = {2},
 issn = {09521976},
 journal = {Engineering Applications of Artificial Intelligence},
 doi = {10.1016/j.engappai.2008.07.004}
}


@article{Vrabie.2009,
 author = {Vrabie, D. and Pastravanu, O. and Abu-Khalaf, M. and Lewis, F. L.},
 year = {2009},
 title = {Adaptive optimal control for continuous-time linear systems based on policy iteration},
 pages = {477--484},
 volume = {45},
 number = {2},
 issn = {00051098},
 journal = {Automatica},
 doi = {10.1016/j.automatica.2008.08.017}
}


@misc{WarrenS.Sarle.1994,
 author = {Sarle, Warren S.},
 year = {1994},
 title = {Neural Network Implementation in SAS{\circledR} Software: Proceedings of the Nineteenth Annual SAS Users Group International Conference},
 institution = {{SAS Institute Inc.}}
}


@incollection{Werbos.2014,
 author = {Werbos, Paul J.},
 title = {NEUROCONTROL AND RELATED TECHNIQUES},
 pages = {345--380},
 publisher = {{Elsevier Science}},
 isbn = {9780125460903},
 editor = {Maren, Alianna J. and Harston, Craig T. and Pap, Robert M.},
 booktitle = {Handbook of Neural Computing Applications},
 year = {2014},
 address = {Burlington},
 doi = {10.1016/B978-0-12-546090-3.50026-7}
}


@book{Widrow.2008,
 abstract = {Preface -- 1. The Adaptive Inverse Control Concept -- 2. Wiener Filters -- 3. Adaptive LMS Filters -- 4. Adaptive Modeling -- 5. Inverse Plant Modeling -- 6. Adaptive Inverse Control -- 7. Other Configurations for Adaptive Inverse Control -- 8. Plant Disturbance Canceling -- 9. System Integration -- 10. Multiple-Input Multiple-Output (MIMO) Adaptive Inverse Control Systems -- 11. Nonlinear Adaptive Inverse Control -- 12. Pleasant Surprises -- A Stability and Misadjustment of the LMS Adaptive Filter -- B Comparative Analyses of Dither Modeling Schemes A, B, C -- C A Comparison of the Self-Tuning Regulator of Astrom and Wittenmark with the Techniques of Adaptive Inverse Control -- D Adaptive Inverse Control for Unstable Linear SISO Plants -- E Orthogonalizing Adaptive Algorithms: RLS, DFT/LMS, and DCT/LMS -- F A MIMO Application: An Adaptive Noise-Canceling System Used for Beam Control at the Stanford Linear Accelerator Center -- G Thirty Years of Adaptive Neural Networks: Perceptron Madaline, and Backpropagation -- H Neural Control Systems -- Glossary -- Index



A self-contained introduction to adaptive inverse control Now featuring a revised preface that emphasizes the coverage of both control systems and signal processing, this reissued edition of Adaptive Inverse Control takes a novel approach that is not available in any other book. Written by two pioneers in the field, Adaptive Inverse Control presents methods of adaptive signal processing that are borrowed from the field of digital signal processing to solve problems in dynamic systems control. This unique approach allows engineers in both fields to share tools and techniques. Clearly and intuitively written, Adaptive Inverse Control illuminates theory with an emphasis on practical applications and commonsense understanding. It covers: the adaptive inverse control concept; Weiner filters; adaptive LMS filters; adaptive modeling; inverse plant modeling; adaptive inverse control; other configurations for adaptive inverse control; plant disturbance canceling; system integration; Multiple-Input Multiple-Output (MIMO) adaptive inverse control systems; nonlinear adaptive inverse control systems; and more. Complete with a glossary, an index, and chapter summaries that consolidate the information presented, Adaptive Inverse Control is appropriate as a textbook for advanced undergraduate- and graduate-level courses on adaptive control and also serves as a valuable resource for practitioners in the fields of control systems and signal processing},
 year = {2008},
 title = {Adaptive inverse control: A signal processing approach},
 keywords = {Acoustical engineering;Adaptive control systems;Adaptive signal processing},
 address = {Piscataway, New Jersey and Hoboken, NJ and Piscataway, New Jersey},
 edition = {Reissue edition},
 publisher = {{IEEE Press Wiley-Interscience} and {IEEE Xplore}},
 isbn = {9780470231616},
 series = {IEEE Press Series on Power Engineering},
 editor = {Widrow, Bernard and Walach, Eugene},
 doi = {10.1002/9780470231616}
}


@article{Yang.2007,
 author = {Yang, Yan-Sheng and Wang, Xiao-Feng},
 year = {2007},
 title = {Adaptive H$\infty$ tracking control for a class of uncertain nonlinear systems using radial-basis-function neural networks},
 pages = {932--941},
 volume = {70},
 number = {4-6},
 issn = {09252312},
 journal = {Neurocomputing},
 doi = {10.1016/j.neucom.2006.10.020}
}


@misc{Zeiler.12222012,
 abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
 author = {Zeiler, Matthew D.},
 date = {12/22/2012},
 title = {ADADELTA: An Adaptive Learning Rate Method},
 url = {http://arxiv.org/pdf/1212.5701v1},
 keywords = {Computer Science - Learning}
}


@article{Zhao.2009,
 author = {Zhao, Bo and Hu, Hongjie},
 year = {2009},
 title = {A new inverse controller for servo--system based on neural network model reference adaptive control},
 pages = {1503--1515},
 volume = {28},
 number = {6},
 issn = {0332-1649},
 journal = {COMPEL - The international journal for computation and mathematics in electrical and electronic engineering},
 doi = {10.1108/03321640910992047}
}


