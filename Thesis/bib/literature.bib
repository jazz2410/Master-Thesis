% This file was created with Citavi 6.1.0.0

@incollection{.2008,
 title = {Appendix G: Thirty Years of Adaptive Neural Networks: Perceptron, Madaline, and Backpropagation},
 pages = {409--474},
 publisher = {{IEEE Press Wiley-Interscience} and {IEEE Xplore}},
 isbn = {9780470231616},
 series = {IEEE Press Series on Power Engineering},
 editor = {Widrow, Bernard and Walach, Eugene},
 booktitle = {Adaptive inverse control},
 year = {2008},
 address = {Piscataway, New Jersey and Hoboken, NJ and Piscataway, New Jersey},
 doi = {10.1002/9780470231616.app7}
}


@book{Alippi.2009,
 year = {2009},
 title = {Artificial neural networks - ICANN 2009: 19th international conference, Limassol, Cyprus, September 14 - 17, 2009 ; proceedings, part II},
 keywords = {Artificial intelligence;Computer science;Computer simulation;Data mining;ilmpub;Neurosciences;Optical pattern recognition},
 address = {Berlin},
 volume = {5769},
 publisher = {Springer},
 isbn = {978-3-642-04276-8},
 series = {Lecture Notes in Computer Science},
 editor = {Alippi, Cesare and Polycarpou, Marios and Panayiotou, Christos and Ellinas, Georgios},
 doi = {10.1007/978-3-642-04277-5}
}


@book{Arkun.1991,
 year = {1991},
 title = {Chemical process control-CPCIV: Proceedings of the Fourth International Conference on Chemical Process Control, Padre Island, Texas, February 17-22, 1991},
 keywords = {Chemical;Congresses},
 address = {Austin Tex.},
 volume = {P-67},
 publisher = {CACHE},
 isbn = {0816905495},
 series = {AIChE publication},
 editor = {Arkun, Yaman and Ray, Willis Harmon}
}


@proceedings{Barrett.1991,
 year = {1991},
 title = {Proceedings of the conference on Analysis of neural network applications},
 keywords = {Computer science},
 address = {New York, NY},
 publisher = {ACM},
 isbn = {0897914325},
 editor = {Barrett, Larry K.},
 institution = {{ACM Special Interest Group on Management Information Systems} and {ACM Special Interest Group on Artificial Intelligence}},
 doi = {10.1145/106965}
}


@incollection{Bayer.2009,
 author = {Bayer, Justin and Wierstra, Daan and Togelius, Julian and Schmidhuber, J{\"u}rgen},
 title = {Evolving Memory Cell Structures for Sequence Learning},
 pages = {755--764},
 volume = {5769},
 publisher = {Springer},
 isbn = {978-3-642-04276-8},
 series = {Lecture Notes in Computer Science},
 editor = {Alippi, Cesare and Polycarpou, Marios and Panayiotou, Christos and Ellinas, Georgios},
 booktitle = {Artificial neural networks - ICANN 2009},
 year = {2009},
 address = {Berlin},
 doi = {10.1007/978-3-642-04277-5{\textunderscore }76}
}


@book{Bishop.2010,
 author = {Bishop, Christopher M.},
 year = {2010},
 title = {Neural networks for pattern recognition},
 keywords = {Mustererkennung;Neuronales Netz;Statistik},
 address = {Oxford},
 edition = {Reprinted.},
 publisher = {{Oxford Univ. Press}},
 isbn = {9780198538646}
}


@misc{Dobel.2018,
 author = {D{\"o}bel, Inga and Leis, Miriam and Vogelsang, Manuel-Molina and Neustroev, Dmitry and Petzka, Henning and R{\"u}ping, Stefan and Voss, Angelika and Mengele, Martin and Welz, Juliane},
 year = {2018},
 title = {Maschinelles Lernen: Kompetenzen, Anwendungen und Forschungsbedarf},
 editor = {Fraunhofer-Gesellschaft}
}


@book{Duriez.2017,
 abstract = {Preface -- 1 Introduction -- 1.1 Feedback in engineering and living systems -- 1.2 Benefits of feedback control -- 1.3 Challenges of feedback control -- 1.4 Feedback turbulence control is a grand challenge problem -- 1.5 Nature teaches us the control design -- 1.6 Outline of the book -- 1.7 Exercises -- 2 Machine learning control (MLC) -- 2.1 Methods of machine learning -- 2.2 MLC with genetic programming -- 2.3 Examples -- 2.4 Exercises -- 2.5 Suggested reading -- 2.6 Interview with Professor Marc Schoenauer -- 3 Methods of linear control theory -- 3.1 Linear systems -- 3.2 Full-state feedback -- Linear quadratic regulator (LQR) -- 3.3 Sensor-based state estimation -- Kalman filtering -- 3.4 Sensor-based feedback -- Linear quadratic Gaussian (LQG) -- 3.5 System Identification and Model Reduction -- 3.6 Exercises -- 3.7 Suggested reading -- 4 Benchmarking MLC against linear control -- 4.1 Comparison of MLC with LQR on a linear oscillator -- 4.2 Comparison of MLC with Kalman filter on a noisy linear oscillator -- 4.3 Comparison of MLC with LQG for sensor-based feedback -- 4.4 Modifications for small nonlinearity -- 4.5 Exercises -- 4.6 Interview with Professor Shervin Bagheri -- 5 Taming nonlinear dynamics with MLC -- 5.1 Generalized mean-field system -- 5.2 Machine learning control -- 5.3 Derivation outline for the generalized mean-field model -- 5.4 Alternative control approaches -- 5.5 Exercises -- 5.6 Suggested reading -- 5.7 Interview with Professor Mark N. Glauser -- 6 Taming real world flow control experiments with MLC -- 6.1 Separation control over a backward-facing step -- 6.2 Separation control of turbulent boundary layers -- 6.3 Control of mixing layer growth -- 6.4 Alternative model-based control approaches -- 6.5 Implementation of MLC in experiments -- 6.6 Suggested reading -- 6.7 Interview with Professor David Williams -- 7 MLC tactics and strategy -- 7.1 The ideal flow control experiment -- 7.2 Desiderata of the control problem - from the definition to hardware choices -- 7.3 Time scales of MLC -- 7.4 MLC parameters and convergence -- 7.5 The imperfect experiment -- 8 Future developments -- 8.1 Methodological advances of MLC -- 8.2 System-reduction techniques for MLC - Coping with high-dimensional input and output -- 8.3 Future applications of MLC -- 8.4 Exercises -- 8.5 Interview with Professor Belinda Batten -- Glossary -- Symbols -- Abbreviations -- Matlab{\circledR} Code: OpenMLC -- Bibliography -- Index



This is the first book on a generally applicable control strategy for turbulence and other complex nonlinear systems. The approach of the book employs powerful methods of machine learning for optimal nonlinear control laws. This machine learning control (MLC) is motivated and detailed in Chapters 1 and 2. In Chapter 3, methods of linear control theory are reviewed. In Chapter 4, MLC is shown to reproduce known optimal control laws for linear dynamics (LQR, LQG). In Chapter 5, MLC detects and exploits a strongly nonlinear actuation mechanism of a low-dimensional dynamical system when linear control methods are shown to fail. Experimental control demonstrations from a laminar shear-layer to turbulent boundary-layers are reviewed in Chapter 6, followed by general good practices for experiments in Chapter 7. The book concludes with an outlook on the vast future applications of MLC in Chapter 8. Matlab codes are provided for easy reproducibility of the presented results. The book includes interviews with leading researchers in turbulence control (S. Bagheri, B. Batten, M. Glauser, D. Williams) and machine learning (M. Schoenauer) for a broader perspective. All chapters have exercises and supplemental videos will be available through YouTube},
 author = {Duriez, Thomas and Brunton, Steven L. and Noack, Bernd R.},
 year = {2017},
 title = {Machine Learning Control - Taming Nonlinear Dynamics and Turbulence},
 url = {http://dx.doi.org/10.1007/978-3-319-40624-4},
 address = {Cham and s.l.},
 volume = {116},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-319-40624-4},
 series = {Fluid Mechanics and Its Applications},
 doi = {10.1007/978-3-319-40624-4}
}


@inproceedings{Fumumoto.2017,
 author = {Fumumoto, Yuta and Owaki, Shotaro and Nakamura, Moriya},
 title = {Effect of number of neurons of a neural-network on compensation performance of SPM non-linear waveform distortion},
 pages = {1--2},
 publisher = {IEEE},
 isbn = {978-1-5090-6293-5},
 booktitle = {2017 Opto-Electronics and Communications Conference (OECC) and Photonics Global Conference (PGC)},
 year = {2017},
 address = {Piscataway, NJ},
 doi = {10.1109/OECC.2017.8114880}
}


@article{Gerwin.1993,
 abstract = {To help meet competitive realities operations managers need to know more about the strategic aspects of manufacturing flexibility. This paper takes steps toward meeting that need by critically reviewing the literature and establishing a research agenda for the area. A conceptual model, which places flexibility within a broad context, helps to identify certain assumptions of theoretical studies which need to be challenged. The model also provides a basis for identifying specific flexibility dimensions. The manner in which these dimensions may limit the effectiveness of a manufacturing process, and the problems in operationalizing them are discussed. Focusing next on the neglected area of applied work, concepts are presented for analyzing whether desired amounts of flexibility are being achieved and whether the potential for flexibility built into a manufacturing process is being tapped. Once more, a procedure is outlined for altering a plant's types and amounts of flexibility over time. The research agenda, which grows out of the appraisal of theoretical and applied work, indicates the value in studying generic flexibility strategies, the flexibility dimensions, methods of delivery, ways of evaluating and changing a process's flexibility, and above all measurement problems. The conclusions indicate principles for strategic research, some of which have relevance for the development of mathematical models.},
 author = {Gerwin, Donald},
 year = {1993},
 title = {Manufacturing Flexibility: A Strategic Perspective},
 url = {http://www.jstor.org/stable/2632407},
 pages = {395--410},
 volume = {39},
 number = {4},
 issn = {00251909},
 journal = {Management Science}
}


@article{Groche.2004,
 author = {Groche, P. and Schneider, R.},
 year = {2004},
 title = {Method for the Optimization of Forming Presses for the Manufacturing of Micro Parts},
 pages = {281--284},
 volume = {53},
 number = {1},
 issn = {00078506},
 journal = {CIRP Annals},
 doi = {10.1016/S0007-8506(07)60698-2}
}


@article{Groche.2010,
 author = {Groche, P. and Scheitza, M. and Kraft, M. and Schmitt, S.},
 year = {2010},
 title = {Increased total flexibility by 3D Servo Presses},
 pages = {267--270},
 volume = {59},
 number = {1},
 issn = {00078506},
 journal = {CIRP Annals},
 doi = {10.1016/j.cirp.2010.03.013}
}


@misc{Han.682015,
 abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.},
 author = {Han, Song and Pool, Jeff and Tran, John and Dally, William J.},
 date = {6/8/2015},
 title = {Learning both Weights and Connections for Efficient Neural Networks},
 url = {http://arxiv.org/pdf/1506.02626v3},
 keywords = {Computer Science - Computer Vision and Pattern Recognition;Computer Science - Learning;Computer Science - Neural and Evolutionary Computing}
}


@article{Hassoun.1996,
 author = {Hassoun, M. H.},
 year = {1996},
 title = {Fundamentals of Artificial Neural Networks},
 pages = {906},
 volume = {84},
 number = {6},
 issn = {0018-9219},
 journal = {Proceedings of the IEEE},
 doi = {10.1109/JPROC.1996.503146}
}


@article{Hornik.1989,
 author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
 year = {1989},
 title = {Multilayer feedforward networks are universal approximators},
 pages = {359--366},
 volume = {2},
 number = {5},
 issn = {08936080},
 journal = {Neural Networks},
 doi = {10.1016/0893-6080(89)90020-8}
}


@proceedings{IEEENeuralNetworksCouncilStaff.June1992,
 year = {June 1992},
 title = {IEEE-INNS International Joint Conference on Neural Networks, Baltimore, 1992},
 address = {Piscataway},
 publisher = {IEEE},
 isbn = {0-7803-0559-0},
 institution = {{IEEE, Neural Networks Council Staff} and {IEEE, Society Staff}}
}


@proceedings{IEEENeuralNetworksCouncilStaff.March1993,
 year = {March 1993},
 title = {IEEE International Conference on Neural Networks 1993},
 address = {Piscataway},
 publisher = {IEEE},
 isbn = {0-7803-0999-5},
 institution = {{IEEE, Neural Networks Council Staff} and {IEEE, Institute of Electrical and Electronics Engineers, Inc. Staff}}
}


@article{Isermann.2010,
 author = {Isermann, Rolf},
 year = {2010},
 title = {Modellbasierte {\"U}berwachung und Fehlerdiagnose von kontinuierlichen technischen Prozessen},
 volume = {58},
 number = {6},
 issn = {0178-2312},
 journal = {at - Automatisierungstechnik},
 doi = {10.1524/auto.2010.0846}
}


@book{Isermann.2011,
 author = {Isermann, Rolf and M{\"u}nchhof, Marco},
 year = {2011},
 title = {Identification of Dynamic Systems: An Introduction with Applications},
 url = {http://dx.doi.org/10.1007/978-3-540-78879-9},
 address = {Berlin, Heidelberg},
 publisher = {{Springer-Verlag Berlin Heidelberg}},
 isbn = {9783540788782},
 series = {Advanced Textbooks in Control and Signal Processing},
 doi = {10.1007/978-3-540-78879-9}
}


@misc{JohnDuchi.2010,
 author = {Duchi, John and Hazan, Elad and Singer, Yoram},
 year = {2010},
 title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}
}


@inproceedings{Kalman.June1992,
 author = {Kalman, B. L. and Kwasny, S. C.},
 title = {Why tanh: choosing a sigmoidal function},
 pages = {578--581},
 publisher = {IEEE},
 isbn = {0-7803-0559-0},
 booktitle = {IEEE-INNS International Joint Conference on Neural Networks, Baltimore, 1992},
 year = {June 1992},
 address = {Piscataway},
 doi = {10.1109/IJCNN.1992.227257}
}


@inproceedings{Kollias.March1993,
 author = {Anastassiou and Kollias},
 title = {Adaptive training of multilayer neural networks using a least squares estimation technique},
 pages = {383-390 vol.1},
 publisher = {IEEE},
 isbn = {0-7803-0999-5},
 booktitle = {IEEE International Conference on Neural Networks 1993},
 year = {March 1993},
 address = {Piscataway},
 doi = {10.1109/ICNN.1988.23870}
}


@misc{Lipton.5292015,
 abstract = {Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities. Recurrent neural networks (RNNs) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window. Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them. In recent years, systems based on long short-term memory (LSTM) and bidirectional (BRNN) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a self-contained explication of the state of the art together with a historical perspective and references to primary research.},
 author = {Lipton, Zachary C. and Berkowitz, John and Elkan, Charles},
 date = {5/29/2015},
 title = {A Critical Review of Recurrent Neural Networks for Sequence Learning},
 url = {http://arxiv.org/pdf/1506.00019v4},
 keywords = {Computer Science - Learning;Computer Science - Neural and Evolutionary Computing}
}


@book{Lunze.2013,
 abstract = {Dieses Lehrbuch~{\"u}berzeugt durch seine Didaktik und Stoffauswahl. Die Darstellung zielt auf ein tiefgr{\"u}ndiges Verst{\"a}ndnis dynamischer Systeme und Regelungsvorg{\"a}nge, wobei mit Zeitbereichsbetrachtungen im Zustandsraum begonnen und erst danach zur Frequenzbereichsdarstellung {\"u}bergegangen wird. Praktische Beispiele aus Elektrotechnik, Maschinenbau, Verfahrenstechnik und Verkehrstechnik illustrieren die Anwendung der behandelten Methoden und zeigen den fach{\"u}bergreifenden Charakter der Regelungstechnik. Mit der Einf{\"u}hrung in MATLAB (Release R2012a) wird der Anschluss an die rechnergest{\"u}tzte Arbeitsweise der Ingenieure hergestellt. {\"U}bungsaufgaben mit ausf{\"u}hrlichen L{\"o}sungen dienen der Vertiefung des Stoffes. F{\"u}r die neunte Auflage wurden zahlreiche Abbildungen verbessert und die Beschreibung von MATLAB der aktuellen Version angepasst.{\glqq}Das Buch vermittelt in idealer Weise theoretisch begr{\"u}ndetes Verst{\"a}ndnis mit praktischen Gesichtspunkten und Beispielen aus verschiedenen Bereichen.{\grqq} Prof. Dr.-Ing. habil. Dr. h.c. Bernhard Lampe, Universit{\"a}t Rostock {\glqq}Das Buch wird von meinen Studenten und Doktoranden sehr gesch{\"a}tzt, weil es zum einen den Grundlagenstoff klar und vollst{\"a}ndig bringt, zum anderen weiterf{\"u}hrende Themen und Prinzipien in knapper und verst{\"a}ndlicher Form erg{\"a}nzt.{\grqq} Prof. Dr.-Ing. Boris Lohmann, Technische Universit{\"a}t M{\"u}nchen},
 author = {Lunze, Jan},
 year = {2013},
 title = {Regelungstechnik 1: Systemtheoretische Grundlagen, Analyse und Entwurf einschleifiger Regelungen},
 url = {http://dx.doi.org/10.1007/978-3-642-29533-1},
 address = {Berlin},
 edition = {9., {\"u}berarb. Aufl.},
 publisher = {Springer},
 isbn = {9783642295324},
 series = {Springer-Lehrbuch},
 doi = {10.1007/978-3-642-29533-1}
}


@book{Masters.1995,
 author = {Masters, Timothy},
 year = {1995},
 title = {Advanced algorithms for neural networks A C++ sourcebook},
 url = {http://www.loc.gov/catdir/enhancements/fy0706/94043390-b.html},
 keywords = {Algorithmus;C {\textless}Programmiersprache{\textgreater};C++ (Computer program language);Computer algorithms;Neural networks (Computer science);Neuronales Netz;Neuronales Netz / Algorithmus / C++ / Programm},
 address = {New York},
 edition = {1. Dr},
 publisher = {Wiley},
 isbn = {0471105880}
}


@proceedings{OECC.2017,
 year = {2017},
 title = {2017 Opto-Electronics and Communications Conference (OECC) and Photonics Global Conference (PGC)},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-5090-6293-5},
 institution = {OECC and {Photonics Global Conference} and {IEEE Photonics Society} and {Optical Society of America} and {Opto-Electronics and Communications Conference} and PGC}
}


@article{Parekh.2000,
 abstract = {Constructive learning algorithms offer an attractive approach for the incremental construction of near-minimal neural-network architectures for pattern classification. They help overcome the need for ad hoc and often inappropriate choices of network topology in algorithms that search for suitable weights in a priori fixed network architectures. Several such algorithms are proposed in the literature and shown to converge to zero classification errors (under certain assumptions) on tasks that involve learning a binary to binary mapping (i.e., classification problems involving binary-valued input attributes and two output categories). We present two constructive learning algorithms MPyramid-real and MTiling-real that extend the pyramid and tiling algorithms, respectively, for learning real to M-ary mappings (i.e., classification problems involving real-valued input attributes and multiple output classes). We prove the convergence of these algorithms and empirically demonstrate their applicability to practical pattern classification problems. Additionally, we show how the incorporation of a local pruning step can eliminate several redundant neurons from MTiling-real networks.},
 author = {Parekh, R. and Yang, J. and Honavar, V.},
 year = {2000},
 title = {Constructive neural-network learning algorithms for pattern classification},
 pages = {436--451},
 volume = {11},
 number = {2},
 issn = {1045-9227},
 journal = {IEEE transactions on neural networks},
 doi = {10.1109/72.839013}
}


@inproceedings{Piovoso.1991,
 author = {Piovoso, Michael J. and Owens, Aaron J.},
 title = {Neural network process control},
 pages = {84--94},
 publisher = {ACM},
 isbn = {0897914325},
 editor = {Barrett, Larry K.},
 booktitle = {Proceedings of the conference on Analysis of neural network applications},
 year = {1991},
 address = {New York, NY},
 doi = {10.1145/106965.105256}
}


@incollection{Piovoso.1991b,
 author = {Piovoso, Michael J. and Owens, Aaron J.},
 title = {Sensor Data Analysis Using Neural Networks},
 publisher = {CACHE},
 isbn = {0816905495},
 series = {AIChE publication},
 editor = {Arkun, Yaman and Ray, Willis Harmon},
 booktitle = {Chemical process control-CPCIV},
 year = {1991},
 address = {Austin Tex.}
}


@book{Rakowitsch.2018,
 author = {Rakowitsch, Martin},
 year = {2018},
 title = {Modellierung der Dynamik eines Koppelgetriebes am Beispiel der 3D-Servo-Presse: Studienarbeit},
 address = {Darmstadt}
}


@article{Schmoeckel.1991,
 author = {Schmoeckel, Dieter},
 year = {1991},
 title = {Developments in Automation, Flexibilization and Control of Forming Machinery},
 pages = {615--622},
 volume = {40},
 number = {2},
 issn = {00078506},
 journal = {CIRP Annals},
 doi = {10.1016/S0007-8506(07)61137-8}
}


@misc{Sinz.2018,
 author = {Sinz, Julian},
 editor = {{Technische Universit{\"a}t Darmstadt  - Institut f{\"u}r Produktionstechnik und Umformmaschinen}},
 year = {2018},
 title = {Die 3D-Servo-Presse - von der Forschungsversion zur industriellen Standardmaschine},
 url = {https://www.ptu.tu-darmstadt.de/mu_forschung/mu_prozesskettenundanlagen/abgeschlossene_forschungsprojekte_pa/die_3d_servo_presse___von_der_forschungsversion_zur_industriellen_standartmaschine/index.de.jsp},
 address = {Darmstadt},
 urldate = {13.09.2018}
}


@book{Sklyarenko.2002,
 author = {Sklyarenko, Yevgen},
 year = {2002},
 title = {Anwendung neuronmaler Netze zur Gegelung von nichtlinearen Roboterantrieben: Dissertation}
}


@article{Son.1987,
 author = {Son, Young Kyu and Park, Chan S.},
 year = {1987},
 title = {Economic measure of productivity, quality and flexibility in advanced manufacturing systems},
 pages = {193--207},
 volume = {6},
 number = {3},
 issn = {02786125},
 journal = {Journal of Manufacturing Systems},
 doi = {10.1016/0278-6125(87)90018-5}
}


@article{StevenA.Harp.1992,
 abstract = {ABSTRACT

We describe an approach to application-specific neural network

design using genetic algorithms. A genetic algorithm is a robust

optimization method particularly well suited for search spaces that are

high-dimensional, discontinuous and noisy-features that typify the neural

network design problem. Our approach is relevant to virtually all neural

network applications: it is network-model independent and it permits

optimization for arbitrary, user-defined criteria. We have developed an

experimental system, NeuroGENESYS, and have conducted several experiments

on small-scale problems. Performance improvements over manual

designs have been observed, the interplay between performance

criteria and network design aspects has been demonstrated, and

general design principles have been uncovered.},
 author = {Harp, Steven A. and Samad, Tariq},
 year = {1992},
 title = {Optimizing Neural Networks with Genetic Algorithms},
 pages = {1138--1143},
 volume = {54 pt 2},
 journal = {Proceedings of the American Power Conference}
}


@book{Sturm.2000,
 author = {Sturm, Michael},
 year = {2000},
 title = {Neuronale Netze zur Modellbildung in der Regelungstechnik: Dissertation}
}


@misc{VDMA.2018,
 author = {Anderl, Reiner and Picard, Andr{\'e} and Wang, Y{\"u}bo and Dosch, Steffen and Klee, Benedikt and Bauer, J{\"o}rg},
 editor = {{VDMA Verlag GmbH}},
 year = {2018},
 title = {Leitfaden Industrie 4.0: Orientierungshilfe zur Einf{\"u}hrung in den Mittelstand},
 url = {https://industrie40.vdma.org/viewer/-/v2article/render/15540546},
 address = {Frankfurt am Main},
 urldate = {14.09.2018}
}


@misc{Velickovic.2018,
 author = {Veli{\v{c}}kovi{\'c}, Petar},
 year = {2018},
 title = {: Multilayer perceptron~(MLP): A diagram representing an in-depth view at a single perceptron, along with its position within a larger-scale multilayer perceptron ({\~{}}unrestricted feedforward neural network).},
 url = {https://github.com/PetarV-/TikZ/tree/master/Multilayer%20perceptron},
 urldate = {21.09.2018}
}


@misc{WarrenS.Sarle.1994,
 author = {Sarle, Warren S.},
 year = {1994},
 title = {Neural Network Implementation in SAS{\circledR} Software: Proceedings of the Nineteenth Annual SAS Users Group International Conference},
 institution = {{SAS Institute Inc.}}
}


@book{Widrow.2008,
 abstract = {Preface -- 1. The Adaptive Inverse Control Concept -- 2. Wiener Filters -- 3. Adaptive LMS Filters -- 4. Adaptive Modeling -- 5. Inverse Plant Modeling -- 6. Adaptive Inverse Control -- 7. Other Configurations for Adaptive Inverse Control -- 8. Plant Disturbance Canceling -- 9. System Integration -- 10. Multiple-Input Multiple-Output (MIMO) Adaptive Inverse Control Systems -- 11. Nonlinear Adaptive Inverse Control -- 12. Pleasant Surprises -- A Stability and Misadjustment of the LMS Adaptive Filter -- B Comparative Analyses of Dither Modeling Schemes A, B, C -- C A Comparison of the Self-Tuning Regulator of Astrom and Wittenmark with the Techniques of Adaptive Inverse Control -- D Adaptive Inverse Control for Unstable Linear SISO Plants -- E Orthogonalizing Adaptive Algorithms: RLS, DFT/LMS, and DCT/LMS -- F A MIMO Application: An Adaptive Noise-Canceling System Used for Beam Control at the Stanford Linear Accelerator Center -- G Thirty Years of Adaptive Neural Networks: Perceptron Madaline, and Backpropagation -- H Neural Control Systems -- Glossary -- Index



A self-contained introduction to adaptive inverse control Now featuring a revised preface that emphasizes the coverage of both control systems and signal processing, this reissued edition of Adaptive Inverse Control takes a novel approach that is not available in any other book. Written by two pioneers in the field, Adaptive Inverse Control presents methods of adaptive signal processing that are borrowed from the field of digital signal processing to solve problems in dynamic systems control. This unique approach allows engineers in both fields to share tools and techniques. Clearly and intuitively written, Adaptive Inverse Control illuminates theory with an emphasis on practical applications and commonsense understanding. It covers: the adaptive inverse control concept; Weiner filters; adaptive LMS filters; adaptive modeling; inverse plant modeling; adaptive inverse control; other configurations for adaptive inverse control; plant disturbance canceling; system integration; Multiple-Input Multiple-Output (MIMO) adaptive inverse control systems; nonlinear adaptive inverse control systems; and more. Complete with a glossary, an index, and chapter summaries that consolidate the information presented, Adaptive Inverse Control is appropriate as a textbook for advanced undergraduate- and graduate-level courses on adaptive control and also serves as a valuable resource for practitioners in the fields of control systems and signal processing},
 year = {2008},
 title = {Adaptive inverse control: A signal processing approach},
 keywords = {Acoustical engineering;Adaptive control systems;Adaptive signal processing},
 address = {Piscataway, New Jersey and Hoboken, NJ and Piscataway, New Jersey},
 edition = {Reissue edition},
 publisher = {{IEEE Press Wiley-Interscience} and {IEEE Xplore}},
 isbn = {9780470231616},
 series = {IEEE Press Series on Power Engineering},
 editor = {Widrow, Bernard and Walach, Eugene},
 doi = {10.1002/9780470231616}
}


@misc{Zeiler.12222012,
 abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
 author = {Zeiler, Matthew D.},
 date = {12/22/2012},
 title = {ADADELTA: An Adaptive Learning Rate Method},
 url = {http://arxiv.org/pdf/1212.5701v1},
 keywords = {Computer Science - Learning}
}


